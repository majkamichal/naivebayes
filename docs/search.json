[{"path":"https://majkamichal.github.io/naivebayes/articles/Sparse matrices.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1) Introduction","title":"Sparse Matrices","text":"Starting 0.9.7 version released March 2020, naivebayes R package introduces specialized implementations Naïve Bayes model support sparse matrices. implementations include: multinomial_naive_bayes(): Specifically designed multinomial data, function handles cases features discrete multiple categories (e.g., word counts text classification). bernoulli_naive_bayes(): Ideal binary data, function handles cases features binary (0 1). poisson_naive_bayes() Tailored count data, function suitable situations features represent counts frequencies. gaussian_naive_bayes(): Suitable continuous data, function assumes features follow Gaussian (normal) distribution. Note: nonparametric_naive_bayes() currently support sparse matrices. specialized functions optimized take advantage sparsity, can significantly enhance computational efficiency. leverage capability, users can provide functions matrix class dgCMatrix excellent Matrix1 package. Importantly, new functionality introduced without breaking changes aligns -dependency philosophy naivebayes project. Users can seamlessly incorporate sparse matrices Naïve Bayes modeling workflow, enhancing performance maintaining compatibility existing code.","code":""},{"path":"https://majkamichal.github.io/naivebayes/articles/Sparse matrices.html","id":"usage","dir":"Articles","previous_headings":"","what":"2) Usage","title":"Sparse Matrices","text":"provided example, showcase training Multinomial Naive Bayes model using simulated sparse matrix. code snippet demonstrates steps involved preparing data, training model, making predictions. code, start simulating sparse matrix M approximately 95% sparsity. matrix 100 rows 10 columns, filled random values 0 5. also generate corresponding factor variable y representing class labels. Next, check fraction zeros matrix confirm sparsity level. cast matrix M “dgCMatrix” object M_sparse using Matrix::Matrix() function, specifying sparse = TRUE argument. Afterward, proceed train Multinomial Naive Bayes model mnb using naivebayes::multinomial_naive_bayes() function. provide sparse matrix M_sparse input x, class labels y. laplace = 1 argument used apply Laplace smoothing2 3 model training. Finally, demonstrate making predictions training data using predict() function, passing trained model mnb sparse matrix M_sparse. ’s important note classifier corresponding prediction function automatically recognize sparse matrix require additional parameters. However, ’s worth mentioning dense matrices internally converted dgCMatrix class. required, conversions need explicitly performed user.","code":"# Simulate ~95% sparse matrix cols <- 10 ; rows <- 100 M <- matrix(sample(0:5, rows * cols, TRUE, prob = c(0.95, rep(0.01, 5))), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M)))  # Check fraction of zeros mean(M == 0) ## [1] 0.946 # Cast the matrix to \"dgCMatrix\" object M_sparse <- Matrix::Matrix(M, sparse = TRUE)  ### Train the Multinomial Naive Bayes and predict the training data mnb <- naivebayes::multinomial_naive_bayes(x = M_sparse, y = y, laplace = 1) head(predict(mnb, M_sparse)) ## [1] classA classB classB classB classB classB ## Levels: classA classB"},{"path":"https://majkamichal.github.io/naivebayes/articles/caret, nproc and naivebayes.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1) Introduction","title":"Caret, nproc and naivebayes","text":"naive_bayes() function, powerful tool classification, readily accessible highly regarded Caret package. Additionally, can utilized seamlessly via nproc package, providing users alternative options. concise documentation aims demonstrate basic usage naive_bayes() function Caret nproc packages.","code":""},{"path":"https://majkamichal.github.io/naivebayes/articles/caret, nproc and naivebayes.html","id":"naive-bayes-in-caret","dir":"Articles","previous_headings":"","what":"2) Naive Bayes in Caret","title":"Caret, nproc and naivebayes","text":"first part showcases train Naive Bayes model using naive_bayes() function within caret interface R. provides example prepare data, train Naive Bayes model, perform classification using trained model. section focuses core steps training Naive Bayes model utilizing classification tasks. second part demonstrates process defining tuning grid, performing resampling, finding “optimal” Naive Bayes model using caret package R. outlines define grid tuning parameters, train multiple models different parameter combinations, select best-performing model based resampling techniques. section emphasizes importance tuning Naive Bayes model achieve better performance provides insights parameter selection process. combining two sections, users can gain comprehensive understanding train optimize Naive Bayes models using naive_bayes() function within caret interface R.","code":""},{"path":"https://majkamichal.github.io/naivebayes/articles/caret, nproc and naivebayes.html","id":"model-training-with-caret","dir":"Articles","previous_headings":"2) Naive Bayes in Caret","what":"2.1) Model Training with Caret","title":"Caret, nproc and naivebayes","text":"model trained using caret::train() function, specifying \"naive_bayes\" method setting usepoisson TRUE handle count variable. training model, classification can performed using trained model predict class labels. Additionally, model provides posterior probabilities, can obtained analysis. needed, code allows accessing underlying naive_bayes object, providing access additional information functionalities Naive Bayes model. Overall, code demonstrates process training Naive Bayes model, performing classification, accessing model object analysis using caret interface R.","code":"library(caret, quietly = TRUE) library(naivebayes) ## naivebayes 1.0.0 loaded ## For more information please visit: ## https://majkamichal.github.io/naivebayes/ # Load the iris dataset data(iris)  # Prepare the data new <- iris[-c(1,2,3)] set.seed(1) new$Discrete <- sample(LETTERS[1:3], nrow(new), TRUE)  set.seed(1) new$Counts <- c(rpois(50, 1), rpois(50, 2), rpois(50, 10))    # Train the Naive Bayes model with the Caret package naive_bayes_via_caret <- train(Species ~ .,                                 data = new,                                 method = \"naive_bayes\",                                 usepoisson = TRUE)  ## Print the trained model # naive_bayes_via_caret  # Perform classification head(predict(naive_bayes_via_caret, newdata = new)) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica # Get posterior probabilities head(predict(naive_bayes_via_caret, newdata = new, type = \"prob\")) ##      setosa   versicolor    virginica ## 1 1.0000000 2.004949e-08 9.549385e-14 ## 2 1.0000000 4.242500e-08 3.229798e-13 ## 3 1.0000000 2.512936e-08 1.566929e-13 ## 4 0.9999999 5.203335e-08 5.710476e-13 ## 5 1.0000000 2.004949e-08 9.549385e-14 ## 6 0.9999514 4.860925e-05 4.128918e-10 # Access the underlying naive_bayes object nb_object <- naive_bayes_via_caret$finalModel class(nb_object) ## [1] \"naive_bayes\" # nb_object"},{"path":"https://majkamichal.github.io/naivebayes/articles/caret, nproc and naivebayes.html","id":"parameter-tuning-with-caret","dir":"Articles","previous_headings":"2) Naive Bayes in Caret","what":"2.2) Parameter Tuning with Caret","title":"Caret, nproc and naivebayes","text":"First, tuning grid defined using expand.grid() function, specifies different combinations tuning parameters Naive Bayes model. , Naive Bayes model fitted parameter tuning using caret::train() function. tuneGrid argument used provide defined tuning grid. resulting model stored naive_bayes_via_caret2 object. view selected tuning parameters, can access naive_bayes_via_caret2$finalModel$tuneValue. uncomment respective line code, can also view final Naive Bayes model . tuning process can visualized using generic plot() function, provides insights performance different parameter combinations. Finally, trained model used perform classification new data using predict() function. class labels new data can obtained predict(naive_bayes_via_caret2, newdata = new).  Overall, code showcases steps involved defining tuning grid, performing model training parameter tuning, visualizing tuning process, applying trained model classification using Naive Bayes algorithm within caret package R.","code":"# Define tuning grid  nb_grid <- expand.grid(usekernel = c(TRUE, FALSE),                        laplace = c(0, 0.5, 1),                         adjust = c(0.75, 1, 1.25, 1.5))  # Fit the Naive Bayes model with parameter tuning set.seed(2550) naive_bayes_via_caret2 <- train(Species ~ .,                                  data = new,                                  method = \"naive_bayes\",                                 usepoisson = TRUE,                                 tuneGrid = nb_grid)  # View the selected tuning parameters naive_bayes_via_caret2$finalModel$tuneValue ##    laplace usekernel adjust ## 16       0      TRUE    1.5 ## View the final naive_bayes model # naive_bayes_via_caret2$finalModel  # Visualize the tuning process plot(naive_bayes_via_caret2) # Perform classification  head(predict(naive_bayes_via_caret2, newdata = new)) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica"},{"path":"https://majkamichal.github.io/naivebayes/articles/caret, nproc and naivebayes.html","id":"naive-bayes-in-nproc","dir":"Articles","previous_headings":"","what":"3) Naive Bayes in nproc","title":"Caret, nproc and naivebayes","text":"nproc package provides Neyman-Pearson (NP) classification algorithms NP receiver operating characteristic (NP-ROC) curves, can used conjunction naivebayes package. incorporating naivebayes package within NP classification framework, users can leverage power naive Bayes algorithm binary classification tasks controlling type type II errors effectively. integration allows practitioners apply NP paradigm naive Bayes classifier enhance performance various application domains. provided code demonstrates usage nproc naivebayes packages R. First, necessary packages loaded. , synthetic data generated using set.seed() function ensure reproducibility. data consists two independent variables x binary response variable y. test dataset xtest also generated evaluation. Next, naive Bayes classifier applied using npc() function nproc package, method set \"nb\" naive Bayes. alpha parameter set 0.05, defining desirable upper bound type error. training classifier, predictions made test dataset using predict() function. accuracy predictions calculated comparing predicted labels nb_pred$pred.label true labels ytest. Additionally, type type II errors computed comparing predicted labels true labels separately class 0 class 1 instances. type error corresponds misclassifying class 0 observations class 1, type II error represents misclassifying class 1 observations class 0. Finally, overall accuracy, type error, type II error printed console using cat() function. code demonstrates leverage nproc naivebayes packages apply Neyman-Pearson classification paradigm1 naive Bayes classifier evaluate performance terms accuracy error rates.","code":"# Example usage with nproc and naivebayes packages  # install.packages(\"nproc\") library(nproc) library(naivebayes)  # Simulate data set.seed(2550) n <- 1000 x <- matrix(rnorm(n * 2), n, 2) c <- 1 + 3 * x[ ,1] y <- rbinom(n, 1, 1 / (1 + exp(-c))) xtest <- matrix(rnorm(n * 2), n, 2) ctest <- 1 + 3 * xtest[,1] ytest <- rbinom(n, 1, 1 / (1 + exp(-ctest)))   # Train Naive Bayes classifier such that the type I error alpha is no bigger than 5% naive_bayes_via_nproc <- npc(x, y, method = \"nb\", alpha = 0.05)  ## Recover the \"naive_bayes\" object # naive_bayes_via_nproc$fits[[1]]$fit  # Classification nb_pred <- predict(naive_bayes_via_nproc, xtest)  # head(nb_pred$pred.label)  # Obtain various measures accuracy <- mean(nb_pred$pred.label == ytest) ind0 <- which(ytest == 0) ind1 <- which(ytest == 1) typeI <- mean(nb_pred$pred.label[ind0] != ytest[ind0])  # type I error on test set typeII <- mean(nb_pred$pred.label[ind1] != ytest[ind1]) # type II error on test set  cat(\" Overall Accuracy: \",  accuracy,\"\\n\",     \"Type I error:     \", typeI, \"\\n\",     \"Type II error:    \", typeII, \"\\n\") ##  Overall Accuracy:  0.689  ##  Type I error:      0.02590674  ##  Type II error:     0.490228"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1) Introduction","title":"","text":"naivebayes package provides user friendly implementation Naïve Bayes algorithm via formula interlace classical combination matrix/data.frame containing features vector class labels. functions can recognize missing values, give informative warnings importantly - know handle . following basic usage main function naive_bayes() demonstrated. Examples specialized Naive Bayes classifiers can found article.","code":""},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"example-data","dir":"Articles","previous_headings":"","what":"2) Example data","title":"","text":"demonstrate usage naivebayes package, use example dataset. dataset simulated consists various variables, including class label, binary variable, categorical variable, logical variable, normally distributed variable, count variable. dataset contains 100 observations, split training set (train) consisting first 95 observations test set (test) consisting remaining 5 observations. test set excludes class label variable (class) simulate new data classification.","code":"library(naivebayes) ## naivebayes 1.0.0 loaded ## For more information please visit: ## https://majkamichal.github.io/naivebayes/ # Simulate example data n <- 100 set.seed(1) data <- data.frame(class = sample(c(\"classA\", \"classB\"), n, TRUE),                    bern = sample(LETTERS[1:2], n, TRUE),                    cat  = sample(letters[1:3], n, TRUE),                    logical = sample(c(TRUE,FALSE), n, TRUE),                    norm = rnorm(n),                    count = rpois(n, lambda = c(5,15))) train <- data[1:95, ] test <- data[96:100, -1]  # Show first and last 6 rows in the simulated training dataset head(train) ##    class bern cat logical        norm count ## 1 classA    B   b   FALSE -0.04470914     6 ## 2 classB    B   a    TRUE -1.73321841    16 ## 3 classA    B   b   FALSE  0.00213186     4 ## 4 classA    A   b   FALSE -0.63030033    20 ## 5 classB    A   b   FALSE -0.34096858     7 ## 6 classA    A   c   FALSE -1.15657236    12 tail(train) ##     class bern cat logical        norm count ## 90 classB    A   b   FALSE -1.02454848    16 ## 91 classA    A   a    TRUE  0.32300650     5 ## 92 classA    B   c    TRUE  1.04361246    18 ## 93 classB    A   c    TRUE  0.09907849     7 ## 94 classB    B   b    TRUE -0.45413691     7 ## 95 classA    B   a    TRUE -0.65578185     4 # Show the test dataset print(test) ##     bern cat logical        norm count ## 96     B   c   FALSE -0.03592242    16 ## 97     A   b    TRUE  1.06916146     9 ## 98     B   a   FALSE -0.48397493    13 ## 99     A   b   FALSE -0.12101011     4 ## 100    A   c   FALSE -1.29414000    13"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"formula-interface","dir":"Articles","previous_headings":"","what":"3) Formula interface","title":"","text":"naivebayes package provides convenient formula interface training utilizing Naïve Bayes models. section, demonstrate use formula interface perform various tasks.","code":"# Train the Naïve Bayes model using the formula interface nb <- naive_bayes(class ~ ., train)  # Summarize the trained model summary(nb) ##  ## ================================= Naive Bayes ==================================  ##   ## - Call: naive_bayes.formula(formula = class ~ ., data = train)  ## - Laplace: 0  ## - Classes: 2  ## - Samples: 95  ## - Features: 5  ## - Conditional distributions:  ##     - Bernoulli: 2 ##     - Categorical: 1 ##     - Gaussian: 2 ## - Prior probabilities:  ##     - classA: 0.4842 ##     - classB: 0.5158 ##  ## -------------------------------------------------------------------------------- # Perform classification on the test data predict(nb, test, type = \"class\") ## [1] classA classB classA classA classA ## Levels: classA classB # Equivalent way of performing the classification task nb %class% test ## [1] classA classB classA classA classA ## Levels: classA classB # Obtain posterior probabilities predict(nb, test, type = \"prob\") ##         classA    classB ## [1,] 0.7174638 0.2825362 ## [2,] 0.2599418 0.7400582 ## [3,] 0.6341795 0.3658205 ## [4,] 0.5365311 0.4634689 ## [5,] 0.7186026 0.2813974 # Equivalent way of performing obtaining posterior probabilities nb %prob% test ##         classA    classB ## [1,] 0.7174638 0.2825362 ## [2,] 0.2599418 0.7400582 ## [3,] 0.6341795 0.3658205 ## [4,] 0.5365311 0.4634689 ## [5,] 0.7186026 0.2813974 # Apply helper functions tables(nb, 1) ## --------------------------------------------------------------------------------  ## :: bern (Bernoulli)  ## --------------------------------------------------------------------------------  ##      ## bern    classA    classB ##    A 0.5000000 0.5510204 ##    B 0.5000000 0.4489796 ##  ## -------------------------------------------------------------------------------- get_cond_dist(nb) ##          bern           cat       logical          norm         count  ##   \"Bernoulli\" \"Categorical\"   \"Bernoulli\"    \"Gaussian\"    \"Gaussian\" # Note: By default, all \"numeric\" (integer, double) variables are modeled #       with a Gaussian distribution."},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"matrixdata-frame-and-class-vector","dir":"Articles","previous_headings":"","what":"4) Matrix/data.frame and class vector","title":"","text":"addition formula interface, naivebayes package also allows training Naïve Bayes models using matrix data frame features (X) vector class labels (class). section, demonstrate utilize approach. code , first separate features train dataset excluding first column (contains class labels). features stored matrix data frame X, corresponding class labels stored vector class. Next, train Naïve Bayes model using naive_bayes() function providing feature matrix data frame (x = X) class vector (y = class) input arguments. trained model stored nb2 object. obtain posterior probabilities class test data, use %prob% operator nb2 object. allows us get probabilities without explicitly using predict() function. using matrix/data.frame class vector approach, can directly train Naïve Bayes model without need formula interface, providing flexibility handling data structures.","code":"# Separate the features and class labels X <- train[-1] class <- train$class  # Train the Naïve Bayes model using the matrix/data.frame and class vector nb2 <- naive_bayes(x = X, y = class)  # Obtain posterior probabilities for the test data nb2 %prob% test ##         classA    classB ## [1,] 0.7174638 0.2825362 ## [2,] 0.2599418 0.7400582 ## [3,] 0.6341795 0.3658205 ## [4,] 0.5365311 0.4634689 ## [5,] 0.7186026 0.2813974"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"non-parametric-estimation-for-continuous-features","dir":"Articles","previous_headings":"","what":"5) Non-parametric estimation for continuous features","title":"","text":"Kernel density estimation (KDE) technique can used estimate class conditional densities continuous features Naïve Bayes modeling. default, naive_bayes() function assumes Gaussian distribution continuous features. However, can explicitly request KDE estimation setting parameter usekernel = TRUE. KDE estimation performed using built-density() function R. default, Gaussian smoothing kernel Silverman’s rule thumb bandwidth selector used. following code snippet, demonstrate usage KDE Naïve Bayes modeling:   code, first train Naïve Bayes model using formula interface set usekernel = TRUE enable KDE estimation. use summary() function obtain summary trained model, including information conditional distributions. Next, use plot() method visualize class conditional densities (prob = \"conditional\") marginal densities class (prob = \"marginal\"). provides insights estimated densities continuous features. Additionally, can customize KDE estimation adjusting kernel bandwidth selection. following sections demonstrate (1) change kernel, (2) bandwidth selector, (3) adjust bandwidth:","code":"# Train a Naïve Bayes model with KDE nb_kde <- naive_bayes(class ~ ., train, usekernel = TRUE) summary(nb_kde) ##  ## ================================= Naive Bayes ==================================  ##   ## - Call: naive_bayes.formula(formula = class ~ ., data = train, usekernel = TRUE)  ## - Laplace: 0  ## - Classes: 2  ## - Samples: 95  ## - Features: 5  ## - Conditional distributions:  ##     - Bernoulli: 2 ##     - Categorical: 1 ##     - KDE: 2 ## - Prior probabilities:  ##     - classA: 0.4842 ##     - classB: 0.5158 ##  ## -------------------------------------------------------------------------------- get_cond_dist(nb_kde) ##          bern           cat       logical          norm         count  ##   \"Bernoulli\" \"Categorical\"   \"Bernoulli\"         \"KDE\"         \"KDE\" nb_kde %prob% test ##         classA    classB ## [1,] 0.6497360 0.3502640 ## [2,] 0.2278895 0.7721105 ## [3,] 0.5914831 0.4085169 ## [4,] 0.5877709 0.4122291 ## [5,] 0.7018091 0.2981909 # Plot class conditional densities plot(nb_kde, \"norm\", arg.num = list(legend.cex = 0.9), prob = \"conditional\") # Plot marginal densities for each class plot(nb_kde, \"norm\", arg.num = list(legend.cex = 0.9), prob = \"marginal\")"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"changing-kernel","dir":"Articles","previous_headings":"5) Non-parametric estimation for continuous features","what":"5.1) Changing kernel","title":"","text":"naive_bayes() function, flexibility specify smoothing kernel used KDE using kernel parameter. seven different smoothing kernels available, characteristics effects density estimation. available kernels (referenced help(\"density\")): gaussian: (Default) default kernel assumes Gaussian (normal) distribution. commonly used provides smooth density estimate. epanechnikov: kernel quadratic shape provides localized density estimate compared Gaussian kernel. rectangular: kernel rectangular shape provides simple, non-smooth density estimate. triangular: kernel triangular shape provides moderately smooth density estimate. biweight: kernel quartic shape provides localized density estimate compared Gaussian kernel. cosine: kernel cosine shape provides smooth density estimate. optcosine: kernel optimized version cosine kernel provides slightly localized density estimate. can refer visualizations kernel functions Wikipedia’s Kernel Functions page: https://en.wikipedia.org/wiki/Kernel_%28statistics%29#Kernel_functions_in_common_use. specifying desired kernel using kernel parameter naive_bayes() function, can choose smoothing approach best suits data modeling objectives.","code":"# Change Gaussian kernel to biweight kernel nb_kde_biweight <- naive_bayes(class ~ .,                                 train,                                 usekernel = TRUE,                                kernel = \"biweight\") nb_kde_biweight %prob% test ##         classA    classB ## [1,] 0.6563243 0.3436757 ## [2,] 0.2349626 0.7650374 ## [3,] 0.5916868 0.4083132 ## [4,] 0.5680861 0.4319139 ## [5,] 0.6981859 0.3018141 plot(nb_kde_biweight, \"norm\", arg.num = list(legend.cex = 0.9), prob = \"conditional\")"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"changing-bandwidth-selector","dir":"Articles","previous_headings":"5) Non-parametric estimation for continuous features","what":"5.2) Changing bandwidth selector","title":"","text":"bandwidth selector critical component KDE determines width kernel influences smoothness estimated density function. can specify bandwidth selector using bw parameter. available bandwidth selectors, described based R documentation help(\"bw.nrd0\"): nrd0 (Silverman’s rule--thumb): default bandwidth selector R. estimates bandwidth based Silverman’s rule--thumb, takes account sample size variance data. provides good balance oversmoothing undersmoothing. nrd (variation rule--thumb): variation Silverman’s rule--thumb, adjusts estimate based factor 1.06. slightly conservative nrd0. ucv (unbiased cross-validation): selector uses unbiased cross-validation estimate bandwidth. bcv (biased cross-validation): selector uses biased cross-validation estimate bandwidth. SJ (Sheather & Jones method): selector implements Sheather-Jones plug-method. Different bandwidth selectors can result different levels smoothness estimated densities. can beneficial experiment multiple selectors find appropriate one specific scenario.","code":"nb_kde_SJ <- naive_bayes(class ~ .,                           train,                           usekernel = TRUE,                          bw = \"SJ\") nb_kde_SJ %prob% test ##         classA    classB ## [1,] 0.6125951 0.3874049 ## [2,] 0.1827523 0.8172477 ## [3,] 0.5784133 0.4215867 ## [4,] 0.7032465 0.2967535 ## [5,] 0.6699161 0.3300839 plot(nb_kde_SJ, \"norm\", arg.num = list(legend.cex = 0.9), prob = \"conditional\")"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"adjusting-bandwidth","dir":"Articles","previous_headings":"5) Non-parametric estimation for continuous features","what":"5.3) Adjusting bandwidth","title":"","text":"can adjust bandwidth specifying factor using adjust. allows additionally control smoothness estimated densities.","code":"nb_kde_adjust <- naive_bayes(class ~ .,                               train,                               usekernel = TRUE,                              adjust = 1.5) nb_kde_adjust %prob% test ##         classA    classB ## [1,] 0.6773096 0.3226904 ## [2,] 0.2428150 0.7571850 ## [3,] 0.6080495 0.3919505 ## [4,] 0.5602177 0.4397823 ## [5,] 0.6910385 0.3089615 plot(nb_kde_adjust, \"norm\", arg.num = list(legend.cex = 0.9), prob = \"conditional\")"},{"path":"https://majkamichal.github.io/naivebayes/articles/naivebayes.html","id":"model-non-negative-integers-with-poisson-distribution","dir":"Articles","previous_headings":"","what":"6) Model non-negative integers with Poisson distribution","title":"","text":"Class conditional distributions non-negative integer predictors can modelled Poisson distribution. can achieved setting usepoisson=TRUE naive_bayes() function making sure variables representing counts dataset class integer.   code snippet checks count variable train dataset class integer. , fits Naïve Bayes model using Poisson distribution specifying usepoisson = TRUE naive_bayes() function.","code":"is.integer(train$count) ## [1] TRUE nb_pois <- naive_bayes(class ~ ., train, usepoisson = TRUE) summary(nb_pois) ##  ## ================================= Naive Bayes ==================================  ##   ## - Call: naive_bayes.formula(formula = class ~ ., data = train, usepoisson = TRUE)  ## - Laplace: 0  ## - Classes: 2  ## - Samples: 95  ## - Features: 5  ## - Conditional distributions:  ##     - Bernoulli: 2 ##     - Categorical: 1 ##     - Poisson: 1 ##     - Gaussian: 1 ## - Prior probabilities:  ##     - classA: 0.4842 ##     - classB: 0.5158 ##  ## -------------------------------------------------------------------------------- get_cond_dist(nb_pois) ##          bern           cat       logical          norm         count  ##   \"Bernoulli\" \"Categorical\"   \"Bernoulli\"    \"Gaussian\"     \"Poisson\" nb_pois %prob% test ##         classA    classB ## [1,] 0.6708181 0.3291819 ## [2,] 0.2792804 0.7207196 ## [3,] 0.6214784 0.3785216 ## [4,] 0.5806921 0.4193079 ## [5,] 0.7074807 0.2925193 # Class conditional distributions plot(nb_pois, \"count\", prob = \"conditional\") # Marginal distributions plot(nb_pois, \"count\", prob = \"marginal\")"},{"path":"https://majkamichal.github.io/naivebayes/articles/specialized_naive_bayes.html","id":"introduction","dir":"Articles","previous_headings":"","what":"1) Introduction","title":"Specialized Naive Bayes","text":"naivebayes package offers comprehensive set functions implement specialized versions Naïve Bayes classifier. article, explore functions provide detailed overview basic usage. package provides five key functions cater different types data: bernoulli_naive_bayes(): Ideal binary data, function specifically tailored handle situations features binary nature, taking values either 0 1. effectively models relationship binary features class labels. multinomial_naive_bayes(): function specifically designed multinomial data. well-suited cases features discrete multiple categories. example, commonly used text classification tasks, word counts utilized features. poisson_naive_bayes(): function specifically designed count data. well-suited scenarios features represent counts frequencies. assuming Poisson distribution features, function effectively models relationship counts class labels. gaussian_naive_bayes(): Suitable continuous data, function assumes features follow Gaussian (normal) distribution. commonly used dealing continuous numerical features, models relationship features class labels. nonparametric_naive_bayes() function introduces nonparametric variant Naïve Bayes classifier. Unlike specialized versions mentioned , make assumptions regarding specific distribution continuous features. Instead, employs kernel density estimation estimate class conditional probabilities feature distributions nonparametric manner. allows flexible modeling can particularly useful dealing data deviates typical parametric assumptions. Throughout article, dive functions providing illustrative examples demonstrate practical usage. end, solid understanding leverage specialized versions Naïve Bayes classifier tackle various types data classification problems. also show equivalent calculations using general naive_bayes() function, allowing compare understand differences specialized general approaches.","code":""},{"path":"https://majkamichal.github.io/naivebayes/articles/specialized_naive_bayes.html","id":"bernoulli-naive-bayes","dir":"Articles","previous_headings":"","what":"2) Bernoulli Naive Bayes","title":"Specialized Naive Bayes","text":"Bernoulli Naive Bayes classifier suitable binary data. demonstrate usage, start simulating dataset binary features. train Bernoulli Naive Bayes model using bernoulli_naive_bayes() function. Simulate data: Train Bernoulli Naive Bayes model:  Equivalent calculation naive_bayes function:","code":"library(naivebayes) ## naivebayes 1.0.0 loaded ## For more information please visit: ## https://majkamichal.github.io/naivebayes/ cols <- 10 ; rows <- 100 ; probs <- c(\"0\" = 0.4, \"1\" = 0.1) M <- matrix(sample(0:1, rows * cols, TRUE, probs), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3, 0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0.5 bnb <- bernoulli_naive_bayes(x = M, y = y, laplace = laplace) # M has to be a matrix summary(bnb) ##  ## ============================ Bernoulli Naive Bayes =============================  ##   ## - Call: bernoulli_naive_bayes(x = M, y = y, laplace = laplace)  ## - Laplace: 0.5  ## - Classes: 2  ## - Samples: 100  ## - Features: 10  ## - Prior probabilities:  ##     - classA: 0.36 ##     - classB: 0.64 ##  ## -------------------------------------------------------------------------------- head(predict(bnb, newdata = M, type = \"prob\")) ##         classA    classB ## [1,] 0.1536234 0.8463766 ## [2,] 0.3689116 0.6310884 ## [3,] 0.4345206 0.5654794 ## [4,] 0.2376475 0.7623525 ## [5,] 0.3954596 0.6045404 ## [6,] 0.2230504 0.7769496 # Equivalently head(bnb %prob% M) ##         classA    classB ## [1,] 0.1536234 0.8463766 ## [2,] 0.3689116 0.6310884 ## [3,] 0.4345206 0.5654794 ## [4,] 0.2376475 0.7623525 ## [5,] 0.3954596 0.6045404 ## [6,] 0.2230504 0.7769496 # Visualise marginal distributions plot(bnb, which = \"V1\", prob = \"marginal\") # Obtain model coefficients coef(bnb) ##      classA:0   classA:1  classB:0  classB:1 ## V1  0.8243243 0.17567568 0.8692308 0.1307692 ## V2  0.6081081 0.39189189 0.7461538 0.2538462 ## V3  0.8513514 0.14864865 0.7153846 0.2846154 ## V4  0.6351351 0.36486486 0.7615385 0.2384615 ## V5  0.7702703 0.22972973 0.8384615 0.1615385 ## V6  0.7432432 0.25675676 0.8230769 0.1769231 ## V7  0.7432432 0.25675676 0.7923077 0.2076923 ## V8  0.8513514 0.14864865 0.7307692 0.2692308 ## V9  0.8513514 0.14864865 0.8384615 0.1615385 ## V10 0.9324324 0.06756757 0.7461538 0.2538462 # It is made sure that the columns are factors with the 0-1 levels) df <- as.data.frame(lapply(as.data.frame(M), factor, levels = c(0, 1))) # sapply(df, class) nb <- naive_bayes(df, y, laplace = laplace) head(nb %prob% df) ##         classA    classB ## [1,] 0.1536234 0.8463766 ## [2,] 0.3689116 0.6310884 ## [3,] 0.4345206 0.5654794 ## [4,] 0.2376475 0.7623525 ## [5,] 0.3954596 0.6045404 ## [6,] 0.2230504 0.7769496"},{"path":"https://majkamichal.github.io/naivebayes/articles/specialized_naive_bayes.html","id":"multinomial-naive-bayes","dir":"Articles","previous_headings":"","what":"3) Multinomial Naive Bayes","title":"Specialized Naive Bayes","text":"Next, explore Multinomial Naive Bayes classifier. simulate dataset multinomial features train model using multinomial_naive_bayes() function. discuss summary model, perform classification, compute posterior probabilities, examine parameter estimates. Note specialized model available general naive_bayes() function. Simulate data: Train Multinomial Naive Bayes:","code":"set.seed(1) cols <- 3 # words rows <- 10000 # all documents rows_spam <- 100 # spam documents  word_prob_non_spam <- prop.table(runif(cols)) word_prob_spam <- prop.table(runif(cols))  M1 <- t(rmultinom(rows_spam, size = cols, prob = word_prob_spam)) M2 <- t(rmultinom(rows - rows_spam, size = cols, prob = word_prob_non_spam)) M <- rbind(M1, M2) colnames(M) <- paste0(\"word\", 1:cols) ; rownames(M) <- paste0(\"doc\", 1:rows) head(M) ##      word1 word2 word3 ## doc1     3     0     0 ## doc2     2     0     1 ## doc3     0     0     3 ## doc4     1     1     1 ## doc5     1     1     1 ## doc6     1     1     1 y <- c(rep(\"spam\", rows_spam), rep(\"non-spam\", rows - rows_spam)) laplace <- 0.5 mnb <- multinomial_naive_bayes(x = M, y = y, laplace = laplace) summary(mnb) ##  ## =========================== Multinomial Naive Bayes ============================  ##   ## - Call: multinomial_naive_bayes(x = M, y = y, laplace = laplace)  ## - Laplace: 0.5  ## - Classes: 2  ## - Samples: 10000  ## - Features: 3  ## - Prior probabilities:  ##     - non-spam: 0.99 ##     - spam: 0.01 ##  ## -------------------------------------------------------------------------------- # Classification head(predict(mnb, M)) # head(mnb %class% M) ## [1] non-spam non-spam non-spam non-spam non-spam non-spam ## Levels: non-spam spam # Posterior probabilities head(predict(mnb, M, type = \"prob\")) # head(mnb %prob% M) ##       non-spam        spam ## doc1 0.9181347 0.081865335 ## doc2 0.9621482 0.037851834 ## doc3 0.9923996 0.007600361 ## doc4 0.9928194 0.007180558 ## doc5 0.9928194 0.007180558 ## doc6 0.9928194 0.007180558 # Parameter estimates coef(mnb) ##        non-spam      spam ## word1 0.2190630 0.4527363 ## word2 0.3099002 0.1177446 ## word3 0.4710368 0.4295191 # Compare estimates to the true probabilities round(cbind(non_spam = word_prob_non_spam, spam = word_prob_spam), 4) ##      non_spam   spam ## [1,]   0.2193 0.4522 ## [2,]   0.3074 0.1004 ## [3,]   0.4732 0.4473"},{"path":"https://majkamichal.github.io/naivebayes/articles/specialized_naive_bayes.html","id":"poisson-naive-bayes","dir":"Articles","previous_headings":"","what":"4) Poisson Naive Bayes","title":"Specialized Naive Bayes","text":"Poisson Naive Bayes classifier specifically designed count data. simulate count data train model using poisson_naive_bayes() function. analyze summary model, perform prediction, visualize marginal distributions, obtain model coefficients. Simulate data: Train Poisson Naive Bayes:  Equivalent calculation naive_bayes function:","code":"cols <- 10 ; rows <- 100 M <- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols) # is.integer(M) # [1] TRUE y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE)) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0 pnb <- poisson_naive_bayes(x = M, y = y, laplace = laplace) summary(pnb) ##  ## ============================= Poisson Naive Bayes ==============================  ##   ## - Call: poisson_naive_bayes(x = M, y = y, laplace = laplace)  ## - Laplace: 0  ## - Classes: 2  ## - Samples: 100  ## - Features: 10  ## - Prior probabilities:  ##     - classA: 0.5 ##     - classB: 0.5 ##  ## -------------------------------------------------------------------------------- head(predict(pnb, newdata = M, type = \"prob\")) ##         classA    classB ## [1,] 0.2940922 0.7059078 ## [2,] 0.4930894 0.5069106 ## [3,] 0.3974324 0.6025676 ## [4,] 0.6535412 0.3464588 ## [5,] 0.6698435 0.3301565 ## [6,] 0.4771426 0.5228574 # Visualise marginal distributions plot(pnb, which = \"V1\", prob = \"marginal\") # Obtain model coefficients coef(pnb) ##     classA classB ## V1    2.84   2.90 ## V2    2.74   2.90 ## V3    2.54   3.06 ## V4    3.12   2.68 ## V5    2.98   3.12 ## V6    2.84   3.20 ## V7    3.26   2.78 ## V8    3.24   3.08 ## V9    3.06   2.96 ## V10   2.76   2.96 nb2 <- naive_bayes(M, y, usepoisson = TRUE, laplace = laplace) head(predict(nb2, type = \"prob\")) ##         classA    classB ## [1,] 0.2940922 0.7059078 ## [2,] 0.4930894 0.5069106 ## [3,] 0.3974324 0.6025676 ## [4,] 0.6535412 0.3464588 ## [5,] 0.6698435 0.3301565 ## [6,] 0.4771426 0.5228574"},{"path":"https://majkamichal.github.io/naivebayes/articles/specialized_naive_bayes.html","id":"gaussian-naive-bayes","dir":"Articles","previous_headings":"","what":"5) Gaussian Naive Bayes","title":"Specialized Naive Bayes","text":"Gaussian Naive Bayes classifier discussed next. use famous Iris1 dataset train Gaussian Naive Bayes model using gaussian_naive_bayes() function. summarize model, visualize class conditional distributions, obtain parameter estimates. Data: Train Gaussian Naive Bayes:  Equivalent calculation general naive_bayes function:","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5]) gnb <- gaussian_naive_bayes(x = M, y = y) summary(gnb) ##  ## ============================= Gaussian Naive Bayes =============================  ##   ## - Call: gaussian_naive_bayes(x = M, y = y)  ## - Samples: 150  ## - Features: 4  ## - Prior probabilities:  ##     - setosa: 0.3333 ##     - versicolor: 0.3333 ##     - virginica: 0.3333 ##  ## -------------------------------------------------------------------------------- head(predict(gnb, newdata = M, type = \"prob\")) ##      setosa   versicolor    virginica ## [1,]      1 2.981309e-18 2.152373e-25 ## [2,]      1 3.169312e-17 6.938030e-25 ## [3,]      1 2.367113e-18 7.240956e-26 ## [4,]      1 3.069606e-17 8.690636e-25 ## [5,]      1 1.017337e-18 8.885794e-26 ## [6,]      1 2.717732e-14 4.344285e-21 # Visualise class conditional distributions plot(gnb, which = \"Sepal.Width\", prob = \"conditional\") # Obtain parameter estimates coef(gnb) ##              setosa:mu setosa:sd versicolor:mu versicolor:sd virginica:mu ## Sepal.Length     5.006 0.3524897         5.936     0.5161711        6.588 ## Sepal.Width      3.428 0.3790644         2.770     0.3137983        2.974 ## Petal.Length     1.462 0.1736640         4.260     0.4699110        5.552 ## Petal.Width      0.246 0.1053856         1.326     0.1977527        2.026 ##              virginica:sd ## Sepal.Length    0.6358796 ## Sepal.Width     0.3224966 ## Petal.Length    0.5518947 ## Petal.Width     0.2746501 coef(gnb)[c(TRUE,FALSE)] # Only means ##              setosa:mu versicolor:mu virginica:mu ## Sepal.Length     5.006         5.936        6.588 ## Sepal.Width      3.428         2.770        2.974 ## Petal.Length     1.462         4.260        5.552 ## Petal.Width      0.246         1.326        2.026 nb3 <- naive_bayes(M, y) head(predict(nb3, newdata = M, type = \"prob\")) ##      setosa   versicolor    virginica ## [1,]      1 2.981309e-18 2.152373e-25 ## [2,]      1 3.169312e-17 6.938030e-25 ## [3,]      1 2.367113e-18 7.240956e-26 ## [4,]      1 3.069606e-17 8.690636e-25 ## [5,]      1 1.017337e-18 8.885794e-26 ## [6,]      1 2.717732e-14 4.344285e-21"},{"path":"https://majkamichal.github.io/naivebayes/articles/specialized_naive_bayes.html","id":"non-parametric-naive-bayes","dir":"Articles","previous_headings":"","what":"6) Non-Parametric Naive Bayes","title":"Specialized Naive Bayes","text":"Lastly, explore Non-Parametric Naive Bayes classifier. , using Iris2 dataset, train model using nonparametric_naive_bayes() function. visualize class conditional distributions, perform classification, can compared Gaussian Naive Bayes demonstrated previous section. Data: Train Non-Parametric Naive Bayes:  Equivalent calculation general naive_bayes function:","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5]) nnb <- nonparametric_naive_bayes(x = M, y = y) # summary(nnb) head(predict(nnb, newdata = M, type = \"prob\")) ##      setosa   versicolor    virginica ## [1,]      1 3.009873e-09 8.846394e-11 ## [2,]      1 4.792767e-08 1.329911e-09 ## [3,]      1 1.950981e-08 1.132901e-09 ## [4,]      1 1.129719e-08 6.470675e-10 ## [5,]      1 8.715390e-10 8.467287e-11 ## [6,]      1 3.746571e-09 5.848304e-09 plot(nnb, which = \"Sepal.Width\", prob = \"conditional\") nb4 <- naive_bayes(M, y, usekernel = TRUE) head(predict(nb4, newdata = M, type = \"prob\")) ##      setosa   versicolor    virginica ## [1,]      1 3.009873e-09 8.846394e-11 ## [2,]      1 4.792767e-08 1.329911e-09 ## [3,]      1 1.950981e-08 1.132901e-09 ## [4,]      1 1.129719e-08 6.470675e-10 ## [5,]      1 8.715390e-10 8.467287e-11 ## [6,]      1 3.746571e-09 5.848304e-09"},{"path":"https://majkamichal.github.io/naivebayes/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michal Majka. Author, maintainer.","code":""},{"path":"https://majkamichal.github.io/naivebayes/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Majka M (2024). naivebayes: High Performance Implementation Naive Bayes Algorithm R. R package version 1.0.0, https://CRAN.R-project.org/package=naivebayes.","code":"@Manual{,   title = {naivebayes: High Performance Implementation of the Naive Bayes Algorithm in R},   author = {Michal Majka},   year = {2024},   note = {R package version 1.0.0},   url = {https://CRAN.R-project.org/package=naivebayes}, }"},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"High Performance Implementation of the Naive Bayes Algorithm","text":"naivebayes package presents efficient implementation widely-used Naïve Bayes classifier. upholds three core principles: efficiency, user-friendliness, reliance solely Base R. adhering latter principle, package ensures stability reliability without introducing external dependencies1. design choice maintains efficiency leveraging optimized routines inherent Base R, many programmed high-performance languages like C/C++ FORTRAN. following principles, naivebayes package provides reliable efficient tool Naïve Bayes classification tasks, ensuring users can perform analyses effectively ease. naive_bayes() function designed determine class feature dataset, depending user specifications, can assume various distributions feature. currently supports following class conditional distributions: categorical distribution discrete features (Bernoulli distribution special case binary outcomes) Poisson distribution non-negative integer features Gaussian distribution continuous features non-parametrically estimated densities via Kernel Density Estimation continuous features addition specialized functions available implement: Bernoulli Naive Bayes via bernoulli_naive_bayes() Multinomial Naive Bayes via multinomial_naive_bayes() Poisson Naive Bayes via poisson_naive_bayes() Gaussian Naive Bayes via gaussian_naive_bayes() Non-Parametric Naive Bayes via nonparametric_naive_bayes() specialized functions carefully optimized efficiency, utilizing linear algebra operations excel handling dense matrices. Additionally, can also exploit sparsity matrices enhanced performance work presence missing data. package also includes various helper functions improve user experience. Moreover, users can access general naive_bayes() function excellent Caret package, providing additional versatility.","code":""},{"path":"https://majkamichal.github.io/naivebayes/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"High Performance Implementation of the Naive Bayes Algorithm","text":"naivebayes package can installed CRAN repository simply executing console following line:","code":"install.packages(\"naivebayes\")  # Or the the development version from GitHub: devtools::install_github(\"majkamichal/naivebayes\")"},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"bernoulli_naive_bayes used fit Bernoulli Naive Bayes model class conditional distributions assumed Bernoulli independent.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"","code":"bernoulli_naive_bayes(x, y, prior = NULL, laplace = 0, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"x matrix numeric 0-1 predictors (matrix dgCMatrix Matrix package). y class vector (character/factor/logical). prior vector prior probabilities classes. unspecified, class proportions training set used. present, probabilities specified order factor levels. laplace value used Laplace smoothing (additive smoothing). Defaults 0 (Laplace smoothing). ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"bernoulli_naive_bayes returns object class \"bernoulli_naive_bayes\" list following components: data list two components: x (matrix predictors) y (class variable). levels character vector values class variable. laplace amount Laplace smoothing (additive smoothing). prob1 matrix class conditional probabilities value 1. Based matrix full probability tables can constructed. Please, see tables coef. prior numeric vector prior probabilities. call call produced object.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"specialized version Naive Bayes classifier, features take numeric 0-1 values class conditional probabilities modelled Bernoulli distribution. Bernoulli Naive Bayes available , naive_bayes bernoulli_naive_bayes. latter provides efficient performance though. Faster calculation times come restricting data numeric 0-1 matrix taking advantage linear algebra operations. Sparse matrices class \"dgCMatrix\" (Matrix package) supported order furthermore speed calculation times. bernoulli_naive_bayes naive_bayes() equivalent latter uses \"0\"-\"1\" character matrix. missing values (NAs) omited constructing probability tables. Also, corresponding predict function excludes NAs calculation posterior probabilities  (informative warning always given).","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/bernoulli_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bernoulli Naive Bayes Classifier — bernoulli_naive_bayes","text":"","code":"# library(naivebayes)  ### Simulate the data: set.seed(1) cols <- 10 ; rows <- 100 ; probs <- c(\"0\" = 0.9, \"1\" = 0.1) M <- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0  ### Train the Bernoulli Naive Bayes bnb <- bernoulli_naive_bayes(x = M, y = y, laplace = laplace) summary(bnb) #>  #> ============================ Bernoulli Naive Bayes =============================  #>   #> - Call: bernoulli_naive_bayes(x = M, y = y, laplace = laplace)  #> - Laplace: 0  #> - Classes: 2  #> - Samples: 100  #> - Features: 10  #> - Prior probabilities:  #>     - classA: 0.28 #>     - classB: 0.72 #>  #> --------------------------------------------------------------------------------   # Classification head(predict(bnb, newdata = M, type = \"class\")) # head(bnb %class% M) #> [1] classB classB classB classB classB classB #> Levels: classA classB  # Posterior probabilities head(predict(bnb, newdata = M, type = \"prob\")) # head(bnb %prob% M) #>          classA    classB #> [1,] 0.08558455 0.9144154 #> [2,] 0.11094760 0.8890524 #> [3,] 0.19887370 0.8011263 #> [4,] 0.39836587 0.6016341 #> [5,] 0.24260388 0.7573961 #> [6,] 0.38489861 0.6151014  # Parameter estimates coef(bnb) #>      classA:0   classA:1  classB:0   classB:1 #> V1  0.9642857 0.03571429 0.9305556 0.06944444 #> V2  0.8214286 0.17857143 0.9444444 0.05555556 #> V3  0.8571429 0.14285714 0.9027778 0.09722222 #> V4  0.8928571 0.10714286 0.8750000 0.12500000 #> V5  0.8928571 0.10714286 0.8472222 0.15277778 #> V6  0.8571429 0.14285714 0.9027778 0.09722222 #> V7  0.8571429 0.14285714 0.8888889 0.11111111 #> V8  0.9642857 0.03571429 0.8750000 0.12500000 #> V9  0.8571429 0.14285714 0.8611111 0.13888889 #> V10 0.8928571 0.10714286 0.8888889 0.11111111   ### Sparse data: train the Bernoulli Naive Bayes library(Matrix) M_sparse <- Matrix(M, sparse = TRUE) class(M_sparse) # dgCMatrix #> [1] \"dgCMatrix\" #> attr(,\"package\") #> [1] \"Matrix\"  # Fit the model with sparse data bnb_sparse <- bernoulli_naive_bayes(M_sparse, y, laplace = laplace)  # Classification head(predict(bnb_sparse, newdata = M_sparse, type = \"class\")) #> [1] classB classB classB classB classB classB #> Levels: classA classB  # Posterior probabilities head(predict(bnb_sparse, newdata = M_sparse, type = \"prob\")) #>            [,1]      [,2] #> [1,] 0.08558455 0.9144154 #> [2,] 0.11094760 0.8890524 #> [3,] 0.19887370 0.8011263 #> [4,] 0.39836587 0.6016341 #> [5,] 0.24260388 0.7573961 #> [6,] 0.38489861 0.6151014  # Parameter estimates coef(bnb_sparse) #>      classA:0   classA:1  classB:0   classB:1 #> V1  0.9642857 0.03571429 0.9305556 0.06944444 #> V2  0.8214286 0.17857143 0.9444444 0.05555556 #> V3  0.8571429 0.14285714 0.9027778 0.09722222 #> V4  0.8928571 0.10714286 0.8750000 0.12500000 #> V5  0.8928571 0.10714286 0.8472222 0.15277778 #> V6  0.8571429 0.14285714 0.9027778 0.09722222 #> V7  0.8571429 0.14285714 0.8888889 0.11111111 #> V8  0.9642857 0.03571429 0.8750000 0.12500000 #> V9  0.8571429 0.14285714 0.8611111 0.13888889 #> V10 0.8928571 0.10714286 0.8888889 0.11111111   ### Equivalent calculation with general naive_bayes function. ### (no sparse data support by naive_bayes)  # Make sure that the columns are factors with the 0-1 levels df <- as.data.frame(lapply(as.data.frame(M), factor, levels = c(0,1))) # sapply(df, class)  nb <- naive_bayes(df, y, laplace = laplace) summary(nb) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.default(x = df, y = y, laplace = laplace)  #> - Laplace: 0  #> - Classes: 2  #> - Samples: 100  #> - Features: 10  #> - Conditional distributions:  #>     - Bernoulli: 10 #> - Prior probabilities:  #>     - classA: 0.28 #>     - classB: 0.72 #>  #> --------------------------------------------------------------------------------  head(predict(nb, type = \"prob\")) #>          classA    classB #> [1,] 0.08558455 0.9144154 #> [2,] 0.11094760 0.8890524 #> [3,] 0.19887370 0.8011263 #> [4,] 0.39836587 0.6016341 #> [5,] 0.24260388 0.7573961 #> [6,] 0.38489861 0.6151014  # Obtain probability tables tables(nb, which = \"V1\") #> --------------------------------------------------------------------------------  #> :: V1 (Bernoulli)  #> --------------------------------------------------------------------------------  #>     #> V1      classA     classB #>   0 0.96428571 0.93055556 #>   1 0.03571429 0.06944444 #>  #> -------------------------------------------------------------------------------- tables(bnb, which = \"V1\") #> --------------------------------------------------------------------------------  #> :: V1 (Bernoulli)  #> --------------------------------------------------------------------------------  #>       classA     classB #> 0 0.96428571 0.93055556 #> 1 0.03571429 0.06944444 #>  #> --------------------------------------------------------------------------------  # Visualise class conditional Bernoulli distributions plot(nb, \"V1\", prob = \"conditional\")  plot(bnb, which = \"V1\", prob = \"conditional\")   # Check the equivalence of the class conditional distributions all(get_cond_dist(nb) == get_cond_dist(bnb)) #> [1] TRUE"},{"path":"https://majkamichal.github.io/naivebayes/reference/coef.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Model Coefficients — coef","title":"Extract Model Coefficients — coef","text":"coef generic function extracts model coefficients specialized Naive Bayes objects.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/coef.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Model Coefficients — coef","text":"","code":"# S3 method for class 'bernoulli_naive_bayes' coef(object, ...)  # S3 method for class 'multinomial_naive_bayes' coef(object, ...)  # S3 method for class 'poisson_naive_bayes' coef(object, ...)  # S3 method for class 'gaussian_naive_bayes' coef(object, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/coef.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Model Coefficients — coef","text":"object object class inheriting \"*_naive_bayes\". ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/coef.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Model Coefficients — coef","text":"Coefficients extracted specialised Naive Bayes objects form data frame.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/coef.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract Model Coefficients — coef","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/coef.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Model Coefficients — coef","text":"","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5])  ### Train the Gaussian Naive Bayes gnb <- gaussian_naive_bayes(x = M, y = y)  ### Extract coefficients coef(gnb) #>              setosa:mu setosa:sd versicolor:mu versicolor:sd virginica:mu #> Sepal.Length     5.006 0.3524897         5.936     0.5161711        6.588 #> Sepal.Width      3.428 0.3790644         2.770     0.3137983        2.974 #> Petal.Length     1.462 0.1736640         4.260     0.4699110        5.552 #> Petal.Width      0.246 0.1053856         1.326     0.1977527        2.026 #>              virginica:sd #> Sepal.Length    0.6358796 #> Sepal.Width     0.3224966 #> Petal.Length    0.5518947 #> Petal.Width     0.2746501  coef(gnb)[c(TRUE,FALSE)] # only means #>              setosa:mu versicolor:mu virginica:mu #> Sepal.Length     5.006         5.936        6.588 #> Sepal.Width      3.428         2.770        2.974 #> Petal.Length     1.462         4.260        5.552 #> Petal.Width      0.246         1.326        2.026  coef(gnb)[c(FALSE,TRUE)] # only standard deviations #>              setosa:sd versicolor:sd virginica:sd #> Sepal.Length 0.3524897     0.5161711    0.6358796 #> Sepal.Width  0.3790644     0.3137983    0.3224966 #> Petal.Length 0.1736640     0.4699110    0.5518947 #> Petal.Width  0.1053856     0.1977527    0.2746501"},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"gaussian_naive_bayes used fit Gaussian Naive Bayes model class conditional distributions assumed Gaussian independent.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"","code":"gaussian_naive_bayes(x, y, prior = NULL, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"x numeric matrix metric predictors (matrix dgCMatrix Matrix package). y class vector (character/factor/logical). prior vector prior probabilities classes. unspecified, class proportions training set used. present, probabilities specified order factor levels. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"gaussian_naive_bayes returns object class \"gaussian_naive_bayes\" list following components: data list two components: x (matrix predictors) y (class variable). levels character vector values class variable. params list two matrices, first containing class conditional means second containing class conditional standard deviations. prior numeric vector prior probabilities. call call produced object.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"specialized version Naive Bayes classifier, features take real values (numeric/integer) class conditional probabilities modelled Gaussian distribution. Gaussian Naive Bayes available , naive_bayes gaussian_naive_bayes.latter provides efficient performance though. Faster calculation times come restricting data matrix numeric columns taking advantage linear algebra operations. Sparse matrices class \"dgCMatrix\" (Matrix package) supported order furthermore speed calculation times. gaussian_naive_bayes naive_bayes() equivalent latter used usepoisson = FALSE usekernel = FALSE; matrix/data.frame contains numeric columns. missing values (NAs) omited estimation process. Also, corresponding predict function excludes NAs calculation posterior probabilities  (informative warning always given).","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/gaussian_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gaussian Naive Bayes Classifier — gaussian_naive_bayes","text":"","code":"# library(naivebayes) set.seed(1) cols <- 10 ; rows <- 100 M <- matrix(rnorm(rows * cols, 100, 15), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M)))   ### Train the Gaussian Naive Bayes gnb <- gaussian_naive_bayes(x = M, y = y) summary(gnb) #>  #> ============================= Gaussian Naive Bayes =============================  #>   #> - Call: gaussian_naive_bayes(x = M, y = y)  #> - Samples: 100  #> - Features: 10  #> - Prior probabilities:  #>     - classA: 0.27 #>     - classB: 0.73 #>  #> --------------------------------------------------------------------------------   # Classification head(predict(gnb, newdata = M, type = \"class\")) # head(gnb %class% M) #> [1] classB classB classA classB classB classB #> Levels: classA classB  # Posterior probabilities head(predict(gnb, newdata = M, type = \"prob\")) # head(gnb %prob% M) #>         classA    classB #> [1,] 0.4553595 0.5446405 #> [2,] 0.3364966 0.6635034 #> [3,] 0.6584491 0.3415509 #> [4,] 0.1223808 0.8776192 #> [5,] 0.1058829 0.8941171 #> [6,] 0.4324113 0.5675887  # Parameter estimates coef(gnb) #>     classA:mu classA:sd classB:mu classB:sd #> V1  100.51459  11.00274 102.04709  14.32595 #> V2   96.56021  14.91150 100.49537  14.11837 #> V3  106.89867  14.50319  98.05817  15.28359 #> V4  101.56819  17.79176 100.48030  13.77000 #> V5   97.61678  19.18794 100.07734  16.98860 #> V6  100.48865  12.21237  98.90448  15.30553 #> V7   97.69133  14.52346  96.75294  16.88204 #> V8  100.75641  15.29154  99.72547  16.96406 #> V9  102.02673  16.07487  99.45745  16.47256 #> V10  98.98690  18.06542 100.45824  15.02574   ### Sparse data: train the Gaussian Naive Bayes library(Matrix) M_sparse <- Matrix(M, sparse = TRUE) class(M_sparse) # dgCMatrix #> [1] \"dgCMatrix\" #> attr(,\"package\") #> [1] \"Matrix\"  # Fit the model with sparse data gnb_sparse <- gaussian_naive_bayes(M_sparse, y)  # Classification head(predict(gnb_sparse, newdata = M_sparse, type = \"class\")) #> [1] classB classB classA classB classB classB #> Levels: classA classB  # Posterior probabilities head(predict(gnb_sparse, newdata = M_sparse, type = \"prob\")) #>         classA    classB #> [1,] 0.4553595 0.5446405 #> [2,] 0.3364966 0.6635034 #> [3,] 0.6584491 0.3415509 #> [4,] 0.1223808 0.8776192 #> [5,] 0.1058829 0.8941171 #> [6,] 0.4324113 0.5675887  # Parameter estimates coef(gnb_sparse) #>     classA:mu classA:sd classB:mu classB:sd #> V1  100.51459  11.00274 102.04709  14.32595 #> V2   96.56021  14.91150 100.49537  14.11837 #> V3  106.89867  14.50319  98.05817  15.28359 #> V4  101.56819  17.79176 100.48030  13.77000 #> V5   97.61678  19.18794 100.07734  16.98860 #> V6  100.48865  12.21237  98.90448  15.30553 #> V7   97.69133  14.52346  96.75294  16.88204 #> V8  100.75641  15.29154  99.72547  16.96406 #> V9  102.02673  16.07487  99.45745  16.47256 #> V10  98.98690  18.06542 100.45824  15.02574   ### Equivalent calculation with general naive_bayes function. ### (no sparse data support by naive_bayes)  nb <- naive_bayes(M, y) summary(nb) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.default(x = M, y = y)  #> - Laplace: 0  #> - Classes: 2  #> - Samples: 100  #> - Features: 10  #> - Conditional distributions:  #>     - Gaussian: 10 #> - Prior probabilities:  #>     - classA: 0.27 #>     - classB: 0.73 #>  #> --------------------------------------------------------------------------------  head(predict(nb, type = \"prob\")) #>         classA    classB #> [1,] 0.4553595 0.5446405 #> [2,] 0.3364966 0.6635034 #> [3,] 0.6584491 0.3415509 #> [4,] 0.1223808 0.8776192 #> [5,] 0.1058829 0.8941171 #> [6,] 0.4324113 0.5675887  # Obtain probability tables tables(nb, which = \"V1\") #> --------------------------------------------------------------------------------  #> :: V1 (Gaussian)  #> --------------------------------------------------------------------------------  #>        #> V1        classA    classB #>   mean 100.51459 102.04709 #>   sd    11.00274  14.32595 #>  #> -------------------------------------------------------------------------------- tables(gnb, which = \"V1\") #> --------------------------------------------------------------------------------  #> :: V1 (Gaussian)  #> --------------------------------------------------------------------------------  #>       classA    classB #> mu 100.51459 102.04709 #> sd  11.00274  14.32595 #>  #> --------------------------------------------------------------------------------  # Visualise class conditional Gaussian distributions plot(nb, \"V1\", prob = \"conditional\")  plot(gnb, which = \"V1\", prob = \"conditional\")   # Check the equivalence of the class conditional distributions all(get_cond_dist(nb) == get_cond_dist(gnb)) #> [1] TRUE"},{"path":"https://majkamichal.github.io/naivebayes/reference/get_cond_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain names of class conditional distribution assigned to features — get_cond_dist","title":"Obtain names of class conditional distribution assigned to features — get_cond_dist","text":"Auxiliary function \"naive_bayes\", \"*_naive_bayes\" \"naive_bayes_tables\"  objects obtaining names class conditional distributions assigned features.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/get_cond_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain names of class conditional distribution assigned to features — get_cond_dist","text":"","code":"get_cond_dist(object)"},{"path":"https://majkamichal.github.io/naivebayes/reference/get_cond_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain names of class conditional distribution assigned to features — get_cond_dist","text":"object object class inheriting \"naive_bayes\" \"*_naive_bayes\" \"naive_bayes_tables\".","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/get_cond_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain names of class conditional distribution assigned to features — get_cond_dist","text":"vector names class conditional distributions assigned features.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/get_cond_dist.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Obtain names of class conditional distribution assigned to features — get_cond_dist","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/get_cond_dist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain names of class conditional distribution assigned to features — get_cond_dist","text":"","code":"data(iris) nb <- naive_bayes(Species ~ ., data = iris) get_cond_dist(nb) # <=> attr(nb$tables, \"cond_dist\") #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>   \"Gaussian\"   \"Gaussian\"   \"Gaussian\"   \"Gaussian\"  get_cond_dist(tables(nb)) #> Sepal.Length  Sepal.Width Petal.Length  Petal.Width  #>   \"Gaussian\"   \"Gaussian\"   \"Gaussian\"   \"Gaussian\""},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for Family of Naive Bayes Objects — Infix operators","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"infix operators %class% %prob% shorthands performing classification obtaining posterior probabilities, respectively.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"","code":"lhs %class% rhs lhs %prob% rhs"},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"lhs object class inheriting \"naive_bayes\" \"*_naive_bayes\" family. rhs dataframe matrix \"naive_bayes\" objects matrix \"*_naive_bayes\" objects.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"%class% returns factor class labels corresponding maximal conditional posterior probabilities. %prob% returns matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"lhs class inheriting family Naive Bayes objects rhs either dataframe matrix infix operators %class% %prob% equivalent : lhs %class% rhs <=> predict(lhs, newdata = rhs, type = \"class\", threshold = 0.001, eps = 0) lhs %prob% rhs <=> predict(lhs, newdata = rhs, type == \"prob\", threshold = 0.001, eps = 0) Compared predict(), operators allow changing values fine tuning parameters threshold eps.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/infix_class_prob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for Family of Naive Bayes Objects — Infix operators","text":"","code":"### Fit the model nb <- naive_bayes(Species ~ ., iris)  newdata <- iris[1:5,-5] # Let's pretend  ### Classification nb %class% newdata #> [1] setosa setosa setosa setosa setosa #> Levels: setosa versicolor virginica predict(nb, newdata, type = \"class\") #> [1] setosa setosa setosa setosa setosa #> Levels: setosa versicolor virginica  ### Posterior probabilities nb %prob% newdata #>      setosa   versicolor    virginica #> [1,]      1 2.981309e-18 2.152373e-25 #> [2,]      1 3.169312e-17 6.938030e-25 #> [3,]      1 2.367113e-18 7.240956e-26 #> [4,]      1 3.069606e-17 8.690636e-25 #> [5,]      1 1.017337e-18 8.885794e-26 predict(nb, newdata, type = \"prob\") #>      setosa   versicolor    virginica #> [1,]      1 2.981309e-18 2.152373e-25 #> [2,]      1 3.169312e-17 6.938030e-25 #> [3,]      1 2.367113e-18 7.240956e-26 #> [4,]      1 3.069606e-17 8.690636e-25 #> [5,]      1 1.017337e-18 8.885794e-26"},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"multinomial_naive_bayes used fit Multinomial Naive Bayes model.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"","code":"multinomial_naive_bayes(x, y, prior = NULL, laplace = 0.5, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"x numeric matrix integer predictors (matrix dgCMatrix Matrix package). y class vector (character/factor/logical). prior vector prior probabilities classes. unspecified, class proportions training set used. present, probabilities specified order factor levels. laplace value used Laplace smoothing (additive smoothing). Defaults 0.5. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"multinomial_naive_bayes returns object class \"multinomial_naive_bayes\" list following components: data list two components: x (matrix predictors) y (class variable). levels character vector values class variable. laplace amount Laplace smoothing (additive smoothing). params matrix class conditional parameter estimates. prior numeric vector prior probabilities. call call produced object.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"specialized version Naive Bayes classifier, features represent frequencies generated multinomial distribution. Sparse matrices class \"dgCMatrix\" (Matrix package) supported order speed calculation times. Please note Multinomial Naive Bayes available naive_bayes function.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"Manning, C.D., Raghavan, P., & Schütze, H. (2008). Introduction Information Retrieval. Cambridge: Cambridge University Press (Chapter 13). Available https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/multinomial_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multinomial Naive Bayes Classifier — multinomial_naive_bayes","text":"","code":"# library(naivebayes)  ### Simulate the data: set.seed(1) cols <- 3 # words rows <- 10000 # all documents rows_spam <- 100 # spam documents  prob_word_non_spam <- prop.table(runif(cols)) prob_word_spam <- prop.table(runif(cols))  M1 <- t(rmultinom(rows_spam, size = cols, prob = prob_word_spam)) M2 <- t(rmultinom(rows - rows_spam, size = cols, prob = prob_word_non_spam)) M <- rbind(M1, M2) colnames(M) <- paste0(\"word\", 1:cols) ; rownames(M) <- paste0(\"doc\", 1:rows) head(M) #>      word1 word2 word3 #> doc1     3     0     0 #> doc2     2     0     1 #> doc3     0     0     3 #> doc4     1     1     1 #> doc5     1     1     1 #> doc6     1     1     1 y <- c(rep(\"spam\", rows_spam), rep(\"non-spam\", rows - rows_spam))  ### Train the Multinomial Naive Bayes laplace <- 1 mnb <- multinomial_naive_bayes(x = M, y = y, laplace = laplace) summary(mnb) #>  #> =========================== Multinomial Naive Bayes ============================  #>   #> - Call: multinomial_naive_bayes(x = M, y = y, laplace = laplace)  #> - Laplace: 1  #> - Classes: 2  #> - Samples: 10000  #> - Features: 3  #> - Prior probabilities:  #>     - non-spam: 0.99 #>     - spam: 0.01 #>  #> --------------------------------------------------------------------------------   # Classification head(predict(mnb, newdata = M, type = \"class\")) # head(mnb %class% M) #> [1] non-spam non-spam non-spam non-spam non-spam non-spam #> Levels: non-spam spam  # Posterior probabilities head(predict(mnb, newdata = M, type = \"prob\")) # head(mnb %prob% M) #>       non-spam        spam #> doc1 0.9184347 0.081565299 #> doc2 0.9622849 0.037715133 #> doc3 0.9924244 0.007575636 #> doc4 0.9927723 0.007227708 #> doc5 0.9927723 0.007227708 #> doc6 0.9927723 0.007227708  # Parameter estimates coef(mnb) #>        non-spam      spam #> word1 0.2190688 0.4521452 #> word2 0.3099014 0.1188119 #> word3 0.4710299 0.4290429  # Compare round(cbind(non_spam = prob_word_non_spam, spam = prob_word_spam), 3) #>      non_spam  spam #> [1,]    0.219 0.452 #> [2,]    0.307 0.100 #> [3,]    0.473 0.447    ### Sparse data: train the Multinomial Naive Bayes library(Matrix) M_sparse <- Matrix(M, sparse = TRUE) class(M_sparse) # dgCMatrix #> [1] \"dgCMatrix\" #> attr(,\"package\") #> [1] \"Matrix\"  # Fit the model with sparse data mnb_sparse <- multinomial_naive_bayes(M_sparse, y, laplace = laplace)  # Classification head(predict(mnb_sparse, newdata = M_sparse, type = \"class\")) #> [1] non-spam non-spam non-spam non-spam non-spam non-spam #> Levels: non-spam spam  # Posterior probabilities head(predict(mnb_sparse, newdata = M_sparse, type = \"prob\")) #>       non-spam        spam #> doc1 0.9184347 0.081565299 #> doc2 0.9622849 0.037715133 #> doc3 0.9924244 0.007575636 #> doc4 0.9927723 0.007227708 #> doc5 0.9927723 0.007227708 #> doc6 0.9927723 0.007227708  # Parameter estimates coef(mnb_sparse) #>        non-spam      spam #> word1 0.2190688 0.4521452 #> word2 0.3099014 0.1188119 #> word3 0.4710299 0.4290429"},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Naive Bayes Classifier — naive_bayes","title":"Naive Bayes Classifier — naive_bayes","text":"naive_bayes used fit Naive Bayes model predictors assumed independent within class label.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Naive Bayes Classifier — naive_bayes","text":"","code":"# Default S3 method naive_bayes(x, y, prior = NULL, laplace = 0,   usekernel = FALSE, usepoisson = FALSE, ...)  # S3 method for class 'formula' naive_bayes(formula, data, prior = NULL, laplace = 0,   usekernel = FALSE, usepoisson = FALSE,   subset, na.action = stats::na.pass, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Naive Bayes Classifier — naive_bayes","text":"x matrix dataframe categorical (character/factor/logical) metric (numeric) predictors. y class vector (character/factor/logical). formula object class \"formula\" (one can coerced \"formula\") form: class ~ predictors (class factor/character/logical). data matrix dataframe categorical (character/factor/logical) metric (numeric) predictors. prior vector prior probabilities classes. unspecified, class proportions training set used. present, probabilities specified order factor levels. laplace value used Laplace smoothing (additive smoothing). Defaults 0 (Laplace smoothing). usekernel logical; TRUE, density used estimate class conditional densities metric predictors. applies vectors class \"numeric\". details interaction usekernel usepoisson parameters please see Note . usepoisson logical; TRUE, Poisson distribution used estimate class conditional PMFs integer predictors (vectors class \"integer\"). subset optional vector specifying subset observations used fitting process. na.action function indicates happen data contain NAs. default (na.pass), missing values removed data omited constructing tables. Alternatively, na.omit can used exclude rows least one missing value constructing tables. ... parameters density usekernel = TRUE (na.rm defaults TRUE) (instance adjust, kernel bw).","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Naive Bayes Classifier — naive_bayes","text":"naive_bayes returns object class \"naive_bayes\" list following components: data list two components: x (dataframe predictors) y (class variable). levels character vector values class variable. laplace amount Laplace smoothing (additive smoothing). tables list tables. categorical predictor table class-conditional probabilities, integer predictor table Poisson mean (usepoisson = TRUE) metric predictor table mean standard deviation density objects class. object tables contains also additional attribute \"cond_dist\" - character vector names conditional distributions assigned feature. prior numeric vector prior probabilities. usekernel logical; TRUE, kernel density estimation used estimating class conditional densities numeric variables. usepoisson logical; TRUE, Poisson distribution used estimating class conditional PMFs non-negative integer variables. call call produced object.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Naive Bayes Classifier — naive_bayes","text":"Numeric (metric) predictors handled assuming follow Gaussian distribution, given class label. Alternatively, kernel density estimation can used (usekernel = TRUE) estimate class-conditional distributions. Also, non-negative integer predictors (variables representing counts) can modelled Poisson distribution (usepoisson = TRUE); details please see Note . Missing values included constructing tables. Logical variables treated categorical (binary) variables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Naive Bayes Classifier — naive_bayes","text":"class \"numeric\" contains \"double\" (double precision floating point numbers) \"integer\". Depending parameters usekernel usepoisson different class conditional distributions applied columns dataset class \"numeric\": usekernel=FALSE poisson=FALSE Gaussian distribution applied \"numeric\" variable (\"numeric\"&\"integer\" \"numeric\"&\"double\") usekernel=TRUE poisson=FALSE kernel density estimation (KDE) applied \"numeric\" variable (\"numeric\"&\"integer\" \"numeric\"&\"double\") usekernel=FALSE poisson=TRUE Gaussian distribution applied \"double\" vector Poisson \"integer\" vector. (Gaussian: \"numeric\" & \"double\"; Poisson: \"numeric\" & \"integer\") usekernel=TRUE poisson=TRUE kernel density estimation (KDE) applied \"double\" vector Poisson \"integer\" vector. (KDE: \"numeric\" & \"double\"; Poisson: \"numeric\" & \"integer\") default usekernel=FALSE poisson=FALSE, thus Gaussian applied numeric variable. hand, \"character\", \"factor\" \"logical\" variables assigned Categorical distribution Bernoulli special case. Prior model fitting classes columns data.frame \"data\" can easily checked via: sapply(data, class) sapply(data, .numeric) sapply(data, .double) sapply(data, .integer)","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Naive Bayes Classifier — naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Naive Bayes Classifier — naive_bayes","text":"","code":"### Simulate example data n <- 100 set.seed(1) data <- data.frame(class = sample(c(\"classA\", \"classB\"), n, TRUE),                    bern = sample(LETTERS[1:2], n, TRUE),                    cat  = sample(letters[1:3], n, TRUE),                    logical = sample(c(TRUE,FALSE), n, TRUE),                    norm = rnorm(n),                    count = rpois(n, lambda = c(5,15))) train <- data[1:95, ] test <- data[96:100, -1]   ### 1) General usage via formula interface nb <- naive_bayes(class ~ ., train) summary(nb) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.formula(formula = class ~ ., data = train)  #> - Laplace: 0  #> - Classes: 2  #> - Samples: 95  #> - Features: 5  #> - Conditional distributions:  #>     - Bernoulli: 2 #>     - Categorical: 1 #>     - Gaussian: 2 #> - Prior probabilities:  #>     - classA: 0.4842 #>     - classB: 0.5158 #>  #> --------------------------------------------------------------------------------   # Classification predict(nb, test, type = \"class\") #> [1] classA classB classA classA classA #> Levels: classA classB nb %class% test #> [1] classA classB classA classA classA #> Levels: classA classB  # Posterior probabilities predict(nb, test, type = \"prob\") #>         classA    classB #> [1,] 0.7174638 0.2825362 #> [2,] 0.2599418 0.7400582 #> [3,] 0.6341795 0.3658205 #> [4,] 0.5365311 0.4634689 #> [5,] 0.7186026 0.2813974 nb %prob% test #>         classA    classB #> [1,] 0.7174638 0.2825362 #> [2,] 0.2599418 0.7400582 #> [3,] 0.6341795 0.3658205 #> [4,] 0.5365311 0.4634689 #> [5,] 0.7186026 0.2813974  # Helper functions tables(nb, 1) #> --------------------------------------------------------------------------------  #> :: bern (Bernoulli)  #> --------------------------------------------------------------------------------  #>      #> bern    classA    classB #>    A 0.5000000 0.5510204 #>    B 0.5000000 0.4489796 #>  #> -------------------------------------------------------------------------------- get_cond_dist(nb) #>          bern           cat       logical          norm         count  #>   \"Bernoulli\" \"Categorical\"   \"Bernoulli\"    \"Gaussian\"    \"Gaussian\"   # Note: all \"numeric\" (integer, double) variables are modelled #       with Gaussian distribution by default.   ### 2) General usage via matrix/data.frame and class vector X <- train[-1] class <- train$class nb2 <- naive_bayes(x = X, y = class) nb2 %prob% test #>         classA    classB #> [1,] 0.7174638 0.2825362 #> [2,] 0.2599418 0.7400582 #> [3,] 0.6341795 0.3658205 #> [4,] 0.5365311 0.4634689 #> [5,] 0.7186026 0.2813974   ### 3) Model continuous variables non-parametrically ###    via kernel density estimation (KDE) nb_kde <- naive_bayes(class ~ ., train, usekernel = TRUE) summary(nb_kde) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.formula(formula = class ~ ., data = train, usekernel = TRUE)  #> - Laplace: 0  #> - Classes: 2  #> - Samples: 95  #> - Features: 5  #> - Conditional distributions:  #>     - Bernoulli: 2 #>     - Categorical: 1 #>     - KDE: 2 #> - Prior probabilities:  #>     - classA: 0.4842 #>     - classB: 0.5158 #>  #> --------------------------------------------------------------------------------  get_cond_dist(nb_kde) #>          bern           cat       logical          norm         count  #>   \"Bernoulli\" \"Categorical\"   \"Bernoulli\"         \"KDE\"         \"KDE\"   nb_kde %prob% test #>         classA    classB #> [1,] 0.6497360 0.3502640 #> [2,] 0.2278895 0.7721105 #> [3,] 0.5914831 0.4085169 #> [4,] 0.5877709 0.4122291 #> [5,] 0.7018091 0.2981909  # Visualize class conditional densities plot(nb_kde, \"norm\", arg.num = list(legend.cex = 0.9), prob = \"conditional\")  plot(nb_kde, \"count\", arg.num = list(legend.cex = 0.9), prob = \"conditional\")   ### ?density and ?bw.nrd for further documentation  # 3.1) Change Gaussian kernel to biweight kernel nb_kde_biweight <- naive_bayes(class ~ ., train, usekernel = TRUE,                                kernel = \"biweight\") nb_kde_biweight %prob% test #>         classA    classB #> [1,] 0.6563243 0.3436757 #> [2,] 0.2349626 0.7650374 #> [3,] 0.5916868 0.4083132 #> [4,] 0.5680861 0.4319139 #> [5,] 0.6981859 0.3018141 plot(nb_kde_biweight, c(\"norm\", \"count\"),      arg.num = list(legend.cex = 0.9), prob = \"conditional\")     # 3.2) Change \"nrd0\" (Silverman's rule of thumb) bandwidth selector nb_kde_SJ <- naive_bayes(class ~ ., train, usekernel = TRUE,                                bw = \"SJ\") nb_kde_SJ %prob% test #>         classA    classB #> [1,] 0.6125951 0.3874049 #> [2,] 0.1827523 0.8172477 #> [3,] 0.5784133 0.4215867 #> [4,] 0.7032465 0.2967535 #> [5,] 0.6699161 0.3300839 plot(nb_kde_SJ, c(\"norm\", \"count\"),      arg.num = list(legend.cex = 0.9), prob = \"conditional\")     # 3.3) Adjust bandwidth nb_kde_adjust <- naive_bayes(class ~ ., train, usekernel = TRUE,                          adjust = 1.5) nb_kde_adjust %prob% test #>         classA    classB #> [1,] 0.6773096 0.3226904 #> [2,] 0.2428150 0.7571850 #> [3,] 0.6080495 0.3919505 #> [4,] 0.5602177 0.4397823 #> [5,] 0.6910385 0.3089615 plot(nb_kde_adjust, c(\"norm\", \"count\"),      arg.num = list(legend.cex = 0.9), prob = \"conditional\")     ### 4) Model non-negative integers with Poisson distribution nb_pois <- naive_bayes(class ~ ., train, usekernel = TRUE, usepoisson = TRUE) summary(nb_pois) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.formula(formula = class ~ ., data = train, usekernel = TRUE,      usepoisson = TRUE)  #> - Laplace: 0  #> - Classes: 2  #> - Samples: 95  #> - Features: 5  #> - Conditional distributions:  #>     - Bernoulli: 2 #>     - Categorical: 1 #>     - Poisson: 1 #>     - KDE: 1 #> - Prior probabilities:  #>     - classA: 0.4842 #>     - classB: 0.5158 #>  #> --------------------------------------------------------------------------------  get_cond_dist(nb_pois) #>          bern           cat       logical          norm         count  #>   \"Bernoulli\" \"Categorical\"   \"Bernoulli\"         \"KDE\"     \"Poisson\"   # Posterior probabilities nb_pois %prob% test #>         classA    classB #> [1,] 0.6675738 0.3324262 #> [2,] 0.2606488 0.7393512 #> [3,] 0.6361172 0.3638828 #> [4,] 0.5774983 0.4225017 #> [5,] 0.7396940 0.2603060  # Class conditional distributions plot(nb_pois, \"count\", prob = \"conditional\")   # Marginal distributions plot(nb_pois, \"count\", prob = \"marginal\")    if (FALSE) { # \\dontrun{ vars <- 10 rows <- 1000000 y <- sample(c(\"a\", \"b\"), rows, TRUE)  # Only categorical variables X1 <- as.data.frame(matrix(sample(letters[5:9], vars * rows, TRUE),                            ncol = vars)) nb_cat <- naive_bayes(x = X1, y = y) nb_cat system.time(pred2 <- predict(nb_cat, X1)) } # }"},{"path":"https://majkamichal.github.io/naivebayes/reference/naivebayes.html","id":null,"dir":"Reference","previous_headings":"","what":"naivebayes — naivebayes","title":"naivebayes — naivebayes","text":"naivebayes package presents efficient implementation widely-used Naive Bayes classifier. upholds three core principles: efficiency, user-friendliness, reliance solely Base R. adhering latter principle, package ensures stability reliability without introducing external dependencies. design choice maintains efficiency leveraging optimized routines inherent Base R, many programmed high-performance languages like C/C++ FORTRAN. following principles, naivebayes package provides reliable efficient tool Naive Bayes classification tasks, ensuring users can perform analyses effectively ease, even presence missing data.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/naivebayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"naivebayes — naivebayes","text":"general naive_bayes() function designed determine class feature dataset, depending user specifications, can assume various distributions feature. currently supports following class conditional distributions: Categorical distribution discrete features (Bernoulli distribution special case binary outcomes) Poisson distribution non-negative integer features Gaussian distribution continuous features non-parametrically estimated densities via Kernel Density Estimation continuous features addition general Naive Bayes function, package provides specialized functions various types Naive Bayes classifiers. specialized functions carefully optimized efficiency, utilizing linear algebra operations excel handling dense matrices. Additionally, can also exploit sparsity matrices enhanced performance: Bernoulli Naiive Bayes via bernoulli_naive_bayes() Multinomial Naive Bayes via multinomial_naive_bayes() Poisson Naive Bayes via poisson_naive_bayes() Gaussian Naive Bayes via gaussian_naive_bayes() Non-Parametric Naive Bayes via nonparametric_naive_bayes() specialized classifiers tailored different assumptions underlying data distributions, offering users versatile tools classification tasks. Moreover, package incorporates various helper functions aimed enhancing user experience. Notably, model fitting functions provided package can effectively handle missing data, ensuring users can utilize classifiers even presence incomplete information. Extended documentation can found website: https://majkamichal.github.io/naivebayes/ Bug reports: https://github.com/majkamichal/naivebayes/issues Contact: michalmajka@hotmail.com","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"nonparametric_naive_bayes used fit Non-Parametric Naive Bayes model class conditional distributions non-parametrically estimated using kernel density estimator assumed independent.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"","code":"nonparametric_naive_bayes(x, y, prior = NULL, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"x matrix metric predictors (numeric matrix accepted). y class vector (character/factor/logical). prior vector prior probabilities classes. unspecified, class proportions training set used. present, probabilities specified order factor levels. ... parameters density (instance adjust, kernel bw).","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"nonparametric_naive_bayes returns object class \"nonparametric_naive_bayes\" list following components: data list two components: x (matrix predictors) y (class variable). levels character vector values class variable. dens nested list containing density objects feature class. prior numeric vector prior probabilities. call call produced object.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"specialized version Naive Bayes classifier, features take real values (numeric/integer) class conditional probabilities estimated non-parametric way kernel density estimator (KDE). default Gaussian kernel used smoothing bandwidth selected according Silverman's 'rule thumb'. details, please see references documentation density bw.nrd0. Non-Parametric Naive Bayes available , naive_bayes() nonparametric_naive_bayes(). latter provide substantial speed general naive_bayes() function meant transparent user friendly. nonparametric_naive_bayes naive_bayes() equivalent latter used usekernel = TRUE usepoisson = FALSE; matrix/data.frame contains numeric variables. missing values (NAs) omitted estimation process. Also, corresponding predict function excludes NAs calculation posterior probabilities  (informative warning always given).","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"Silverman, B. W. (1986). Density Estimation Statistics Data Analysis. Chapman & Hall.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/nonparametric_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Non-Parametric Naive Bayes Classifier — nonparametric_naive_bayes","text":"","code":"# library(naivebayes) data(iris) y <- iris[[5]] M <- as.matrix(iris[-5])  ### Train the Non-Parametric Naive Bayes nnb <- nonparametric_naive_bayes(x = M, y = y) summary(nnb) #>  #> ========================== Nonparametric Naive Bayes ===========================  #>   #> - Call: nonparametric_naive_bayes(x = M, y = y)  #> - Classes: 3  #> - Samples: 150  #> - Features: 4  #> - Prior probabilities:  #>     - setosa: 0.3333 #>     - versicolor: 0.3333 #>     - virginica: 0.3333 #>  #> --------------------------------------------------------------------------------  head(predict(nnb, newdata = M, type = \"prob\")) #>      setosa   versicolor    virginica #> [1,]      1 3.009873e-09 8.846394e-11 #> [2,]      1 4.792767e-08 1.329911e-09 #> [3,]      1 1.950981e-08 1.132901e-09 #> [4,]      1 1.129719e-08 6.470675e-10 #> [5,]      1 8.715390e-10 8.467287e-11 #> [6,]      1 3.746571e-09 5.848304e-09  ###  Equivalent calculation with general naive_bayes function: nb <- naive_bayes(M, y, usekernel = TRUE) summary(nb) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.default(x = M, y = y, usekernel = TRUE)  #> - Laplace: 0  #> - Classes: 3  #> - Samples: 150  #> - Features: 4  #> - Conditional distributions:  #>     - KDE: 4 #> - Prior probabilities:  #>     - setosa: 0.3333 #>     - versicolor: 0.3333 #>     - virginica: 0.3333 #>  #> --------------------------------------------------------------------------------  head(predict(nb, type = \"prob\")) #>      setosa   versicolor    virginica #> [1,]      1 3.009873e-09 8.846394e-11 #> [2,]      1 4.792767e-08 1.329911e-09 #> [3,]      1 1.950981e-08 1.132901e-09 #> [4,]      1 1.129719e-08 6.470675e-10 #> [5,]      1 8.715390e-10 8.467287e-11 #> [6,]      1 3.746571e-09 5.848304e-09  ### Change kernel nnb_kernel <- nonparametric_naive_bayes(x = M, y = y, kernel = \"biweight\") plot(nnb_kernel, 1, prob = \"conditional\")   ### Adjust bandwidth nnb_adjust <- nonparametric_naive_bayes(M, y, adjust = 1.5) plot(nnb_adjust, 1, prob = \"conditional\")   ### Change bandwidth selector nnb_bw <- nonparametric_naive_bayes(M, y, bw = \"SJ\") plot(nnb_bw, 1, prob = \"conditional\")   ### Obtain tables with conditional densities # tables(nnb, which = 1)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.bernoulli_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","title":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","text":"Plot method objects class \"bernoulli_naive_bayes\" designed quick look class marginal distributions class conditional distributions 0-1 valued predictors.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.bernoulli_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","text":"","code":"# S3 method for class 'bernoulli_naive_bayes' plot(x, which = NULL, ask = FALSE, arg.cat = list(),      prob = c(\"marginal\", \"conditional\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.bernoulli_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","text":"x object class inheriting \"bernoulli_naive_bayes\". variables plotted (default). can valid indexing vector vector containing names variables. ask logical; TRUE, user asked plot, see par(ask=.). arg.cat parameters passed named list mosaicplot. prob character; \"marginal\" marginal distributions predictor variables class visualised \"conditional\" class conditional distributions predictor variables depicted. default, prob=\"marginal\". ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.bernoulli_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","text":"Class conditional class conditional distributions visualised mosaicplot. parameter prob controls kind probabilities visualized individual predictor \\(Xi\\). can take two values: \"marginal\": \\(P(Xi|class) * P(class)\\) \"conditional\": \\(P(Xi|class)\\)","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.bernoulli_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.bernoulli_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for bernoulli_naive_bayes Objects — plot.bernoulli_naive_bayes","text":"","code":"# Simulate data cols <- 10 ; rows <- 100 ; probs <- c(\"0\" = 0.4, \"1\" = 0.1) M <- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0.5  # Train the Bernoulli Naive Bayes model bnb <- bernoulli_naive_bayes(x = M, y = y, laplace = laplace)  # Visualize class marginal probabilities corresponding to the first feature plot(bnb, which = 1)   # Visualize class conditional probabilities corresponding to the first feature plot(bnb, which = 1, prob = \"conditional\")"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.gaussian_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","title":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","text":"Plot method objects class \"gaussian_naive_bayes\" designed quick look class marginal conditional Gaussian distributions metric predictors.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.gaussian_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","text":"","code":"# S3 method for class 'gaussian_naive_bayes' plot(x, which = NULL, ask = FALSE, legend = TRUE,   legend.box = FALSE, arg.num = list(),   prob = c(\"marginal\", \"conditional\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.gaussian_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","text":"x object class inheriting \"gaussian_naive_bayes\". variables plotted (default). can valid indexing vector vector containing names variables. ask logical; TRUE, user asked plot, see par(ask=.). legend logical; TRUE legend plotted. legend.box logical; TRUE box drawn around legend. arg.num parameters passed named list matplot. prob character; \"marginal\" marginal distributions predictor variables class visualised \"conditional\" class conditional distributions predictor variables depicted. default, prob=\"marginal\". ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.gaussian_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","text":"Class marginal class conditional Gaussian distributions visualised matplot. parameter prob controls kind probabilities visualized individual predictor \\(Xi\\). can take two values: \"marginal\": \\(P(Xi|class) * P(class)\\) \"conditional\": \\(P(Xi|class)\\)","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.gaussian_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.gaussian_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for gaussian_naive_bayes Objects — plot.gaussian_naive_bayes","text":"","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5])  ### Train the Gaussian Naive Bayes with custom prior gnb <- gaussian_naive_bayes(x = M, y = y, prior = c(0.1,0.3,0.6))  # Visualize class marginal Gaussian distributions corresponding # to the first feature plot(gnb, which = 1)   # Visualize class conditional Gaussian distributions corresponding # to the first feature plot(gnb, which = 1, prob = \"conditional\")"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for naive_bayes Objects — plot.naive_bayes","title":"Plot Method for naive_bayes Objects — plot.naive_bayes","text":"Plot method objects class \"naive_bayes\" designed quick look class marginal distributions class conditional distributions predictor variables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for naive_bayes Objects — plot.naive_bayes","text":"","code":"# S3 method for class 'naive_bayes' plot(x, which = NULL, ask = FALSE, legend = TRUE,   legend.box = FALSE, arg.num = list(), arg.cat = list(),   prob = c(\"marginal\", \"conditional\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for naive_bayes Objects — plot.naive_bayes","text":"x object class inheriting \"naive_bayes\". variables plotted (default). can valid indexing vector vector containing names variables. ask logical; TRUE, user asked plot, see par(ask=.). legend logical; TRUE legend plotted. legend.box logical; TRUE box drawn around legend. arg.num parameters passed named list matplot. arg.cat parameters passed named list mosaicplot. prob character; \"marginal\" marginal distributions predictor variables class visualised \"conditional\" class conditional distributions predictor variables depicted. default, prob=\"marginal\". ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for naive_bayes Objects — plot.naive_bayes","text":"Probabilities visualised matplot (numeric (metric) predictors)  mosaicplot (categorical predictors). case non parametric estimation densities, bandwidths reported class. Nothing returned. numeric (metric) predictors position legend can adjusted changed via arg.num(..., legend.position = \"topright\"). legend.position can one \"topright\" \"topleft\", \"bottomright\", \"bottomleft\". order adjust legend size following argument can used: arg.num(..., legend.cex = 0.9). parameter prob controls kind probabilities visualized individual predictor \\(Xi\\). can take two values: \"marginal\": \\(P(Xi|class) * P(class)\\) \"conditional\": \\(P(Xi|class)\\)","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for naive_bayes Objects — plot.naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for naive_bayes Objects — plot.naive_bayes","text":"","code":"data(iris) iris2 <- cbind(iris, New = sample(letters[1:3], 150, TRUE))  # Fit the model with custom prior probabilities nb <- naive_bayes(Species ~ ., data = iris2, prior = c(0.1, 0.3, 0.6))  # Visualize marginal distributions of two predictors plot(nb, which = c(\"Sepal.Width\", \"Sepal.Length\"), ask = TRUE)    # Visualize class conditional distributions corresponding to the first predictor # with customized settings plot(nb, which = 1, ask = FALSE, prob = \"conditional\",      arg.num = list(col = 1:3, lty = 1,      main = \"Naive Bayes Plot\", legend.position = \"topright\",      legend.cex = 0.55))   # Visualize class marginal distributions corresponding to the first predictor # with customized settings plot(nb, which = 1, ask = FALSE, prob = \"marginal\",      arg.num = list(col = 1:3, lty = 1,      main = \"Naive Bayes Plot\", legend.position = \"topright\",      legend.cex = 0.55))   # Visualize class marginal distribution corresponding to the predictor \"new\" # with custom colours plot(nb, which = \"New\", arg.cat = list(color = gray.colors(3)))"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.nonparametric_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","title":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","text":"Plot method objects class \"nonparametric_naive_bayes\" designed quick look estimated class marginal class conditional densities metric predictors.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.nonparametric_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","text":"","code":"# S3 method for class 'nonparametric_naive_bayes' plot(x, which = NULL, ask = FALSE, legend = TRUE,   legend.box = FALSE, arg.num = list(),   prob = c(\"marginal\", \"conditional\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.nonparametric_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","text":"x object class inheriting \"nonparametric_naive_bayes\". variables plotted (default). can valid indexing vector vector containing names variables. ask logical; TRUE, user asked plot, see par(ask=.). legend logical; TRUE legend plotted. legend.box logical; TRUE box drawn around legend. arg.num parameters passed named list matplot. prob character; \"marginal\" marginal distributions predictor variables class visualised \"conditional\" class conditional distributions predictor variables depicted. default, prob=\"marginal\". ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.nonparametric_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","text":"Estimated class marginal class conditional densities visualised matplot. parameter prob controls kind probabilities visualized individual predictor \\(Xi\\). can take two values: \"marginal\": \\(P(Xi|class) * P(class)\\) \"conditional\": \\(P(Xi|class)\\)","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.nonparametric_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.nonparametric_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for nonparametric_naive_bayes Objects — plot.nonparametric_naive_bayes","text":"","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5])  ### Train the Non-Parametric Naive Bayes with custom prior prior <- c(0.1,0.3,0.6) nnb <- nonparametric_naive_bayes(x = M, y = y, prior = prior) nnb2 <- nonparametric_naive_bayes(x = M, y = y, prior = prior, adjust = 1.5) nnb3 <- nonparametric_naive_bayes(x = M, y = y, prior = prior, bw = \"ucv\") #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range #> Warning: minimum occurred at one end of the range  # Visualize estimated class conditional densities corresponding # to the first feature plot(nnb, which = 1, prob = \"conditional\")  plot(nnb2, which = 1, prob = \"cond\")  plot(nnb3, which = 1, prob = \"c\")   # Visualize estimated class marginal densities corresponding # to the first feature plot(nnb, which = 1)  plot(nnb2, which = 1)  plot(nnb3, which = 1)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.poisson_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","title":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","text":"Plot method objects class \"poisson_naive_bayes\" designed quick look class marginal class conditional Poisson distributions non-negative integer predictors.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.poisson_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","text":"","code":"# S3 method for class 'poisson_naive_bayes' plot(x, which = NULL, ask = FALSE, legend = TRUE,   legend.box = FALSE, arg.num = list(),   prob = c(\"marginal\", \"conditional\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.poisson_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","text":"x object class inheriting \"poisson_naive_bayes\". variables plotted (default). can valid indexing vector vector containing names variables. ask logical; TRUE, user asked plot, see par(ask=.). legend logical; TRUE legend plotted. legend.box logical; TRUE box drawn around legend. arg.num parameters passed named list matplot. prob character; \"marginal\" marginal distributions predictor variables class visualised \"conditional\" class conditional distributions predictor variables depicted. default, prob=\"marginal\". ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.poisson_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","text":"Class marginal class conditional Poisson distributions visualised matplot. parameter prob controls kind probabilities visualized individual predictor \\(Xi\\). can take two values: \"marginal\": \\(P(Xi|class) * P(class)\\) \"conditional\": \\(P(Xi|class)\\)","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.poisson_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/plot.poisson_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for poisson_naive_bayes Objects — plot.poisson_naive_bayes","text":"","code":"cols <- 10 ; rows <- 100 M <- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols) # is.integer(M) # [1] TRUE y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE)) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0  ### Train the Poisson Naive Bayes pnb <- poisson_naive_bayes(x = M, y = y, laplace = laplace)  # Visualize class conditional Poisson distributions corresponding # to the first feature plot(pnb, which = 1, prob = \"conditional\")   # Visualize class marginal Poisson distributions corresponding # to the first feature plot(pnb, which = 1)"},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Poisson Naive Bayes Classifier — poisson_naive_bayes","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"poisson_naive_bayes used fit Poisson Naive Bayes model class conditional distributions assumed Poisson independent.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"","code":"poisson_naive_bayes(x, y, prior = NULL, laplace = 0, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"x numeric matrix integer predictors (matrix dgCMatrix Matrix package). y class vector (character/factor/logical). prior vector prior probabilities classes. unspecified, class proportions training set used. present, probabilities specified order factor levels. laplace value used Laplace smoothing (additive smoothing). Defaults 0 (Laplace smoothing). ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"poisson_naive_bayes returns object class \"poisson_naive_bayes\" list following components: data list two components: x (matrix predictors) y (class variable). levels character vector values class variable. laplace amount Laplace smoothing (additive smoothing). params matrix containing class conditional means. prior numeric vector prior probabilities. call call produced object.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"specialized version Naive Bayes classifier, features take non-negative integers (numeric/integer) class conditional probabilities modelled Poisson distribution. Poisson Naive Bayes available , naive_bayes poisson_naive_bayes. latter provides efficient performance though. Faster calculation times come restricting data integer-valued matrix taking advantage linear algebra operations. Sparse matrices class \"dgCMatrix\" (Matrix package) supported order furthermore speed calculation times. poisson_naive_bayes naive_bayes() equivalent latter used usepoisson = TRUE usekernel = FALSE; matrix/data.frame contains integer-valued columns. missing values (NAs) omited estimation process. Also, corresponding predict function excludes NAs calculation posterior probabilities  (informative warning always given).","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"parameter laplace set positive constant c amount added counts. leads (\"global\") Bayesian estimation improper prior. case, estimate expected value posterior given gamma distribution parameters: cell count + c number observations cell. one cell zero count laplace = 0 one pseudo-count automatically cell. corresponds \"local\" Bayesian estimation uniform prior.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/poisson_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Poisson Naive Bayes Classifier — poisson_naive_bayes","text":"","code":"library(naivebayes)  ### Simulate the data: set.seed(1) cols <- 10 ; rows <- 100 M <- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0.5   ### Train the Poisson Naive Bayes pnb <- poisson_naive_bayes(x = M, y = y, laplace = laplace) summary(pnb) #>  #> ============================= Poisson Naive Bayes ==============================  #>   #> - Call: poisson_naive_bayes(x = M, y = y, laplace = laplace)  #> - Laplace: 0.5  #> - Classes: 2  #> - Samples: 100  #> - Features: 10  #> - Prior probabilities:  #>     - classA: 0.28 #>     - classB: 0.72 #>  #> --------------------------------------------------------------------------------   # Classification head(predict(pnb, newdata = M, type = \"class\")) # head(pnb %class% M) #> [1] classB classB classB classB classB classB #> Levels: classA classB  # Posterior probabilities head(predict(pnb, newdata = M, type = \"prob\")) # head(pnb %prob% M) #>         classA    classB #> [1,] 0.2796812 0.7203188 #> [2,] 0.1973988 0.8026012 #> [3,] 0.2594386 0.7405614 #> [4,] 0.3054088 0.6945912 #> [5,] 0.1748781 0.8251219 #> [6,] 0.2500402 0.7499598  # Parameter estimates coef(pnb) #>       classA   classB #> V1  2.589286 3.243056 #> V2  3.553571 2.923611 #> V3  2.946429 2.576389 #> V4  3.089286 3.159722 #> V5  3.017857 3.118056 #> V6  3.482143 3.006944 #> V7  3.053571 3.118056 #> V8  2.696429 3.145833 #> V9  3.125000 2.881944 #> V10 3.232143 2.965278   ### Sparse data: train the Poisson Naive Bayes library(Matrix) M_sparse <- Matrix(M, sparse = TRUE) class(M_sparse) # dgCMatrix #> [1] \"dgCMatrix\" #> attr(,\"package\") #> [1] \"Matrix\"  # Fit the model with sparse data pnb_sparse <- poisson_naive_bayes(M_sparse, y, laplace = laplace)  # Classification head(predict(pnb_sparse, newdata = M_sparse, type = \"class\")) #> [1] classB classB classB classB classB classB #> Levels: classA classB  # Posterior probabilities head(predict(pnb_sparse, newdata = M_sparse, type = \"prob\")) #>         classA    classB #> [1,] 0.2796812 0.7203188 #> [2,] 0.1973988 0.8026012 #> [3,] 0.2594386 0.7405614 #> [4,] 0.3054088 0.6945912 #> [5,] 0.1748781 0.8251219 #> [6,] 0.2500402 0.7499598  # Parameter estimates coef(pnb_sparse) #>       classA   classB #> V1  2.589286 3.243056 #> V2  3.553571 2.923611 #> V3  2.946429 2.576389 #> V4  3.089286 3.159722 #> V5  3.017857 3.118056 #> V6  3.482143 3.006944 #> V7  3.053571 3.118056 #> V8  2.696429 3.145833 #> V9  3.125000 2.881944 #> V10 3.232143 2.965278   ### Equivalent calculation with general naive_bayes function. ### (no sparse data support by naive_bayes)  nb <- naive_bayes(M, y, laplace = laplace, usepoisson = TRUE) summary(nb) #>  #> ================================= Naive Bayes ==================================  #>   #> - Call: naive_bayes.default(x = M, y = y, laplace = laplace, usepoisson = TRUE)  #> - Laplace: 0.5  #> - Classes: 2  #> - Samples: 100  #> - Features: 10  #> - Conditional distributions:  #>     - Poisson: 10 #> - Prior probabilities:  #>     - classA: 0.28 #>     - classB: 0.72 #>  #> --------------------------------------------------------------------------------  head(predict(nb, type = \"prob\")) #>         classA    classB #> [1,] 0.2796812 0.7203188 #> [2,] 0.1973988 0.8026012 #> [3,] 0.2594386 0.7405614 #> [4,] 0.3054088 0.6945912 #> [5,] 0.1748781 0.8251219 #> [6,] 0.2500402 0.7499598  # Obtain probability tables tables(nb, which = \"V1\") #> --------------------------------------------------------------------------------  #> :: V1 (Poisson)  #> --------------------------------------------------------------------------------  #>  #>          classA   classB #> lambda 2.589286 3.243056 #>  #> -------------------------------------------------------------------------------- tables(pnb, which = \"V1\") #> --------------------------------------------------------------------------------  #> :: V1 (Poisson)  #> --------------------------------------------------------------------------------  #>  #>          classA   classB #> lambda 2.589286 3.243056 #>  #> --------------------------------------------------------------------------------  # Visualise class conditional Poisson distributions plot(nb, \"V1\", prob = \"conditional\") plot(pnb, which = \"V1\", prob = \"conditional\")   # Check the equivalence of the class conditional distributions all(get_cond_dist(nb) == get_cond_dist(pnb)) #> [1] TRUE"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"Classification based Bernoulli Naive Bayes model.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"","code":"# S3 method for class 'bernoulli_naive_bayes' predict(object, newdata = NULL, type = c(\"class\",\"prob\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"object object class inheriting \"bernoulli_naive_bayes\". newdata matrix numeric 0-1 predictors. type \"class\", new data points classified according highest posterior probabilities. \"prob\", posterior probabilities class returned. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"predict.bernoulli_naive_bayes returns either factor class labels corresponding maximal conditional posterior probabilities matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"specialized version Naive Bayes classifier, features take numeric 0-1 values class conditional probabilities modelled Bernoulli distribution. Class posterior probabilities calculated using Bayes' rule assumption independence predictors. newdata provided, data object used. Bernoulli Naive Bayes available , naive_bayes bernoulli_naive_bayes. implementation specialized Naive Bayes provides efficient performance though. speedup comes restricting data input numeric 0-1 matrix performing linear algebra well vectorized operations . words, efficiency comes cost flexibility. NAs newdata included calculation posterior probabilities; present informative warning given. bernoulli_naive_bayes function equivalent naive_bayes function numeric 0-1 matrix coerced, instance,  \"0\"-\"1\" character matrix.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.bernoulli_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for bernoulli_naive_bayes Objects — predict.bernoulli_naive_bayes","text":"","code":"cols <- 10 ; rows <- 100 ; probs <- c(\"0\" = 0.4, \"1\" = 0.1) M <- matrix(sample(0:1, rows * cols,  TRUE, probs), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0.5  ### Train the Bernoulli Naive Bayes bnb <- bernoulli_naive_bayes(x = M, y = y, laplace = laplace)  ### Classification head(predict(bnb, newdata = M, type = \"class\")) #> [1] classB classB classB classB classA classB #> Levels: classA classB head(bnb %class% M) #> [1] classB classB classB classB classA classB #> Levels: classA classB  ### Posterior probabilities head(predict(bnb, newdata = M, type = \"prob\")) #>         classA    classB #> [1,] 0.1178526 0.8821474 #> [2,] 0.1003894 0.8996106 #> [3,] 0.1426711 0.8573289 #> [4,] 0.4601912 0.5398088 #> [5,] 0.5246835 0.4753165 #> [6,] 0.3463304 0.6536696 head(bnb %prob% M) #>         classA    classB #> [1,] 0.1178526 0.8821474 #> [2,] 0.1003894 0.8996106 #> [3,] 0.1426711 0.8573289 #> [4,] 0.4601912 0.5398088 #> [5,] 0.5246835 0.4753165 #> [6,] 0.3463304 0.6536696"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"Classification based Gaussian Naive Bayes model.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"","code":"# S3 method for class 'gaussian_naive_bayes' predict(object, newdata = NULL, type = c(\"class\",\"prob\"),   threshold = 0.001, eps = 0, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"object object class inheriting \"gaussian_naive_bayes\". newdata matrix metric predictors (numeric matrix accepted). type \"class\", new data points classified according highest posterior probabilities. \"prob\", posterior probabilities class returned. threshold value zero probabilities probabilities within epsilon-range corresponding metric variables replaced (zero probabilities corresponding categorical variables can handled Laplace (additive) smoothing). eps value specifies epsilon-range replace zero close zero probabilities threshold. applies metric variables. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"predict.gaussian_naive_bayes returns either factor class labels corresponding maximal conditional posterior probabilities matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"specialized version Naive Bayes classifier, features take real values class conditional probabilities modelled Gaussian distribution. Class posterior probabilities calculated using Bayes' rule assumption independence predictors. newdata provided, data object used. Gaussian Naive Bayes available , naive_bayes gaussian_naive_bayes. implementation specialized Naive Bayes provides efficient performance though. speedup comes restricting data input numeric matrix performing linear algebra well vectorized operations . words, efficiency comes cost flexibility. NAs newdata included calculation posterior probabilities; present informative warning given. gaussian_naive_bayes function equivalent naive_bayes function numeric matrix data.frame containing numeric variables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.gaussian_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for gaussian_naive_bayes Objects — predict.gaussian_naive_bayes","text":"","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5])  ### Train the Gaussian Naive Bayes gnb <- gaussian_naive_bayes(x = M, y = y)  ### Classification head(predict(gnb, newdata = M, type = \"class\")) #> [1] setosa setosa setosa setosa setosa setosa #> Levels: setosa versicolor virginica head(gnb %class% M) #> [1] setosa setosa setosa setosa setosa setosa #> Levels: setosa versicolor virginica  ### Posterior probabilities head(predict(gnb, newdata = M, type = \"prob\")) #>      setosa   versicolor    virginica #> [1,]      1 2.981309e-18 2.152373e-25 #> [2,]      1 3.169312e-17 6.938030e-25 #> [3,]      1 2.367113e-18 7.240956e-26 #> [4,]      1 3.069606e-17 8.690636e-25 #> [5,]      1 1.017337e-18 8.885794e-26 #> [6,]      1 2.717732e-14 4.344285e-21 head(gnb %prob% M) #>      setosa   versicolor    virginica #> [1,]      1 2.981309e-18 2.152373e-25 #> [2,]      1 3.169312e-17 6.938030e-25 #> [3,]      1 2.367113e-18 7.240956e-26 #> [4,]      1 3.069606e-17 8.690636e-25 #> [5,]      1 1.017337e-18 8.885794e-26 #> [6,]      1 2.717732e-14 4.344285e-21"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"Classification based Multinomial Naive Bayes model.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"","code":"# S3 method for class 'multinomial_naive_bayes' predict(object, newdata = NULL, type = c(\"class\",\"prob\"), ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"object object class inheriting \"multinomial_naive_bayes\". newdata matrix non-negative integer predictors (numeric matrix accepted). type \"class\", new data points classified according highest posterior probabilities. \"prob\", posterior probabilities class returned. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"predict.multinomial_naive_bayes returns either factor class labels corresponding maximal conditional posterior probabilities matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"specialized version Naive Bayes classifier, features represent frequencies events generated multinomial distribution. Multinomial Naive Bayes available naive_bayes function. NAs newdata included calculation posterior probabilities; present informative warning given.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"McCallum, Andrew; Nigam, Kamal (1998). comparison event models Naive Bayes text classification (PDF). AAAI-98 workshop learning text categorization. 752. http://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.multinomial_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for multinomial_naive_bayes Objects — predict.multinomial_naive_bayes","text":"","code":"### Simulate the data: cols <- 10 ; rows <- 100 M <- matrix(sample(0:5, rows * cols,  TRUE), nrow = rows, ncol = cols) y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE, prob = c(0.3,0.7))) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 1  ### Train the Multinomial Naive Bayes mnb <- multinomial_naive_bayes(x = M, y = y, laplace = laplace)  # Classification head(predict(mnb, newdata = M, type = \"class\")) #> [1] classB classB classB classB classA classB #> Levels: classA classB head(mnb %class% M) #> [1] classB classB classB classB classA classB #> Levels: classA classB  # Posterior probabilities head(predict(mnb, newdata = M, type = \"prob\")) #>          classA    classB #> [1,] 0.17623129 0.8237687 #> [2,] 0.41834308 0.5816569 #> [3,] 0.09334622 0.9066538 #> [4,] 0.19324231 0.8067577 #> [5,] 0.56491024 0.4350898 #> [6,] 0.23376937 0.7662306 head(mnb %prob% M) #>          classA    classB #> [1,] 0.17623129 0.8237687 #> [2,] 0.41834308 0.5816569 #> [3,] 0.09334622 0.9066538 #> [4,] 0.19324231 0.8067577 #> [5,] 0.56491024 0.4350898 #> [6,] 0.23376937 0.7662306"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for naive_bayes Objects — predict.naive_bayes","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"Classification based Naive Bayes models.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"","code":"# S3 method for class 'naive_bayes' predict(object, newdata = NULL, type = c(\"class\",\"prob\"),   threshold = 0.001, eps = 0, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"object object class inheriting \"naive_bayes\". newdata matrix dataframe categorical (character/factor/logical) metric (numeric) predictors. type \"class\", new data points classified according highest posterior probabilities. \"prob\", posterior probabilities class returned. threshold value zero probabilities probabilities within epsilon-range corresponding metric variables replaced (zero probabilities corresponding categorical variables can handled Laplace (additive) smoothing). eps value specifies epsilon-range replace zero close zero probabilities threshold. applies metric variables. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"predict.naive_bayes returns either factor class labels corresponding maximal conditional posterior probabilities matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"Computes conditional posterior probabilities class label using Bayes' rule assumption independence predictors. new data provided, data object used. Logical variables treated categorical (binary) variables. Predictors missing values included computation posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for naive_bayes Objects — predict.naive_bayes","text":"","code":"### Simulate example data n <- 100 set.seed(1) data <- data.frame(class = sample(c(\"classA\", \"classB\"), n, TRUE),                    bern = sample(LETTERS[1:2], n, TRUE),                    cat  = sample(letters[1:3], n, TRUE),                    logical = sample(c(TRUE,FALSE), n, TRUE),                    norm = rnorm(n),                    count = rpois(n, lambda = c(5,15))) train <- data[1:95, ] test <- data[96:100, -1]  ### Fit the model with default settings nb <- naive_bayes(class ~ ., train)  # Classification predict(nb, test, type = \"class\") #> [1] classA classB classA classA classA #> Levels: classA classB nb %class% test #> [1] classA classB classA classA classA #> Levels: classA classB  # Posterior probabilities predict(nb, test, type = \"prob\") #>         classA    classB #> [1,] 0.7174638 0.2825362 #> [2,] 0.2599418 0.7400582 #> [3,] 0.6341795 0.3658205 #> [4,] 0.5365311 0.4634689 #> [5,] 0.7186026 0.2813974 nb %prob% test #>         classA    classB #> [1,] 0.7174638 0.2825362 #> [2,] 0.2599418 0.7400582 #> [3,] 0.6341795 0.3658205 #> [4,] 0.5365311 0.4634689 #> [5,] 0.7186026 0.2813974   if (FALSE) { # \\dontrun{ vars <- 10 rows <- 1000000 y <- sample(c(\"a\", \"b\"), rows, TRUE)  # Only categorical variables X1 <- as.data.frame(matrix(sample(letters[5:9], vars * rows, TRUE),                            ncol = vars)) nb_cat <- naive_bayes(x = X1, y = y) nb_cat system.time(pred2 <- predict(nb_cat, X1)) } # }"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"Classification based Non-Parametric Naive Bayes model.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"","code":"# S3 method for class 'nonparametric_naive_bayes' predict(object, newdata = NULL, type = c(\"class\",\"prob\"),   threshold = 0.001, eps = 0, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"object object class inheriting \"nonparametric_naive_bayes\". newdata matrix metric predictors (numeric matrix accepted). type \"class\", new data points classified according highest posterior probabilities. \"prob\", posterior probabilities class returned. threshold value zero probabilities probabilities within epsilon-range corresponding metric variables replaced (zero probabilities corresponding categorical variables can handled Laplace (additive) smoothing). eps value specifies epsilon-range replace zero close zero probabilities threshold. applies metric variables. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"predict.nonparametric_naive_bayes returns either factor class labels corresponding maximal conditional posterior probabilities matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"specialized version Naive Bayes classifier, features take real values (numeric/integer) class conditional probabilities non-parametrically estimated kernel density estimator. default Gaussian kernel used smoothing bandwidth selected according Silverman's 'rule thumb'. details, please see references documentation density bw.nrd0. Non-Parametric Naive Bayes available , naive_bayes() nonparametric_naive_bayes(). specialized implementation Naive Bayes provide substantial speed-general naive_bayes() function transparent user friendly. nonparametric_naive_bayes function equivalent naive_bayes() numeric matrix data.frame contains numeric variables usekernel = TRUE. missing values (NAs) omitted parameter estimation. NAs newdata predict.nonparametric_naive_bayes() included calculation posterior probabilities; present informative warning given.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"Silverman, B. W. (1986). Density Estimation Statistics Data Analysis. Chapman & Hall.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.nonparametric_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for nonparametric_naive_bayes Objects — predict.nonparametric_naive_bayes","text":"","code":"data(iris) y <- iris[[5]] M <- as.matrix(iris[-5])  ### Train the Non-Parametric Naive Bayes nnb <- nonparametric_naive_bayes(x = M, y = y, bw = \"SJ\")  ### Classification head(predict(nnb, newdata = M, type = \"class\")) #> [1] setosa setosa setosa setosa setosa setosa #> Levels: setosa versicolor virginica head(nnb %class% M) #> [1] setosa setosa setosa setosa setosa setosa #> Levels: setosa versicolor virginica  ### Posterior probabilities head(predict(nnb, newdata = M, type = \"prob\")) #>      setosa   versicolor    virginica #> [1,]      1 6.001557e-10 2.007251e-11 #> [2,]      1 7.629087e-09 1.821586e-10 #> [3,]      1 3.228899e-09 1.609810e-10 #> [4,]      1 2.284583e-09 1.130646e-10 #> [5,]      1 1.873705e-10 1.568085e-11 #> [6,]      1 5.167803e-10 1.025134e-09 head(nnb %prob% M) #>      setosa   versicolor    virginica #> [1,]      1 6.001557e-10 2.007251e-11 #> [2,]      1 7.629087e-09 1.821586e-10 #> [3,]      1 3.228899e-09 1.609810e-10 #> [4,]      1 2.284583e-09 1.130646e-10 #> [5,]      1 1.873705e-10 1.568085e-11 #> [6,]      1 5.167803e-10 1.025134e-09"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"Classification based Poisson Naive Bayes model.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"","code":"# S3 method for class 'poisson_naive_bayes' predict(object, newdata = NULL, type = c(\"class\",\"prob\"),   threshold = 0.001, eps = 0, ...)"},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"object object class inheriting \"poisson_naive_bayes\". newdata matrix non-negative integer predictors (numeric matrix accepted). type \"class\", new data points classified according highest posterior probabilities. \"prob\", posterior probabilities class returned. threshold value zero probabilities probabilities within epsilon-range corresponding metric variables replaced (zero probabilities corresponding categorical variables can handled Laplace (additive) smoothing). eps value specifies epsilon-range replace zero close zero probabilities threshold. ... used.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"predict.poisson_naive_bayes returns either factor class labels corresponding maximal conditional posterior probabilities matrix class label specific conditional posterior probabilities.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"specialized version Naive Bayes classifier, features non-negative integers class conditional probabilities modelled Poisson distribution. Class posterior probabilities calculated using Bayes' rule assumption independence predictors. newdata provided, data object used. Poisson Naive Bayes available , naive_bayes poisson_naive_bayes. implementation specialized Naive Bayes provides efficient performance though. speedup comes restricting data input numeric matrix performing linear algebra well vectorized operations . NAs newdata included calculation posterior probabilities; present informative warning given. poisson_naive_bayes function equivalent naive_bayes function usepoisson=TRUE numeric matrix data.frame containing non-negative integer valued features (variable class \"integer\").","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/predict.poisson_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for poisson_naive_bayes Objects — predict.poisson_naive_bayes","text":"","code":"cols <- 10 ; rows <- 100 M <- matrix(rpois(rows * cols, lambda = 3), nrow = rows, ncol = cols) # is.integer(M) # [1] TRUE y <- factor(sample(paste0(\"class\", LETTERS[1:2]), rows, TRUE)) colnames(M) <- paste0(\"V\", seq_len(ncol(M))) laplace <- 0  ### Train the Poisson Naive Bayes pnb <- poisson_naive_bayes(x = M, y = y, laplace = laplace)  ### Classification head(predict(pnb, newdata = M, type = \"class\")) #> [1] classB classB classB classB classB classB #> Levels: classA classB head(pnb %class% M) #> [1] classB classB classB classB classB classB #> Levels: classA classB  ### Posterior probabilities head(predict(pnb, newdata = M, type = \"prob\")) #>          classA    classB #> [1,] 0.04297243 0.9570276 #> [2,] 0.30693453 0.6930655 #> [3,] 0.17136603 0.8286340 #> [4,] 0.09829519 0.9017048 #> [5,] 0.44241727 0.5575827 #> [6,] 0.37354122 0.6264588 head(pnb %prob% M) #>          classA    classB #> [1,] 0.04297243 0.9570276 #> [2,] 0.30693453 0.6930655 #> [3,] 0.17136603 0.8286340 #> [4,] 0.09829519 0.9017048 #> [5,] 0.44241727 0.5575827 #> [6,] 0.37354122 0.6264588"},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Browse Tables of Naive Bayes Classifier — tables","title":"Browse Tables of Naive Bayes Classifier — tables","text":"Auxiliary function \"naive_bayes\" \"*_naive_bayes\" objects easy browsing tables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Browse Tables of Naive Bayes Classifier — tables","text":"","code":"tables(object, which = NULL)"},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Browse Tables of Naive Bayes Classifier — tables","text":"object object class inheriting : \"naive_bayes\" \"*_naive_bayes\". tables showed (default). can valid indexing vector vector containing names variables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Browse Tables of Naive Bayes Classifier — tables","text":"list tables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Browse Tables of Naive Bayes Classifier — tables","text":"Default print method \"naive_bayes\" \"*_naive_bayes\" objects shows five first tables. auxiliary function tables returns default tables allows easy subsetting via indexing variables.","code":""},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Browse Tables of Naive Bayes Classifier — tables","text":"Michal Majka, michalmajka@hotmail.com","code":""},{"path":[]},{"path":"https://majkamichal.github.io/naivebayes/reference/tables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Browse Tables of Naive Bayes Classifier — tables","text":"","code":"data(iris) nb <- naive_bayes(Species ~ ., data = iris) tables(nb, \"Sepal.Length\") #> --------------------------------------------------------------------------------  #> :: Sepal.Length (Gaussian)  #> --------------------------------------------------------------------------------  #>              #> Sepal.Length    setosa versicolor virginica #>         mean 5.0060000  5.9360000 6.5880000 #>         sd   0.3524897  0.5161711 0.6358796 #>  #> -------------------------------------------------------------------------------- tables(nb, c(\"Sepal.Length\", \"Sepal.Width\")) #> --------------------------------------------------------------------------------  #> :: Sepal.Length (Gaussian)  #> --------------------------------------------------------------------------------  #>              #> Sepal.Length    setosa versicolor virginica #>         mean 5.0060000  5.9360000 6.5880000 #>         sd   0.3524897  0.5161711 0.6358796 #>  #> --------------------------------------------------------------------------------  #> :: Sepal.Width (Gaussian)  #> --------------------------------------------------------------------------------  #>             #> Sepal.Width    setosa versicolor virginica #>        mean 3.4280000  2.7700000 2.9740000 #>        sd   0.3790644  0.3137983 0.3224966 #>  #> -------------------------------------------------------------------------------- tabs <- tables(nb, 1:2) tabs #> --------------------------------------------------------------------------------  #> :: Sepal.Length (Gaussian)  #> --------------------------------------------------------------------------------  #>              #> Sepal.Length    setosa versicolor virginica #>         mean 5.0060000  5.9360000 6.5880000 #>         sd   0.3524897  0.5161711 0.6358796 #>  #> --------------------------------------------------------------------------------  #> :: Sepal.Width (Gaussian)  #> --------------------------------------------------------------------------------  #>             #> Sepal.Width    setosa versicolor virginica #>        mean 3.4280000  2.7700000 2.9740000 #>        sd   0.3790644  0.3137983 0.3224966 #>  #> -------------------------------------------------------------------------------- tabs[1] #> --------------------------------------------------------------------------------  #> :: Sepal.Length (Gaussian)  #> --------------------------------------------------------------------------------  #>              #> Sepal.Length    setosa versicolor virginica #>         mean 5.0060000  5.9360000 6.5880000 #>         sd   0.3524897  0.5161711 0.6358796 #>  #> --------------------------------------------------------------------------------"},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-100","dir":"Changelog","previous_headings":"","what":"naivebayes 1.0.0","title":"naivebayes 1.0.0","text":"CRAN release: 2024-03-16","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"major-release-maturity-and-stability-1-0-0","dir":"Changelog","previous_headings":"","what":"Major Release: Maturity and Stability","title":"naivebayes 1.0.0","text":"package reached significant milestone maturity stability, leading version update 1.0.0. Improvement: enhanced print methods. Improvement: updated documentation. Improvement: minor internal enhancements.","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-097","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.7","title":"naivebayes 0.9.7","text":"CRAN release: 2020-03-08 Improvement: multinomial_naive_bayes(), bernoulli_naive_bayes(), poisson_naive_bayes() gaussian_naive_bayes() now support sparse matrices (dgCMatrix class Matrix Package). Improvement: updated documentation. Improvement: better informative errors.","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-096","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.6","title":"naivebayes 0.9.6","text":"CRAN release: 2019-06-03","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"improvements-0-9-6","dir":"Changelog","previous_headings":"","what":"Improvements:","title":"naivebayes 0.9.6","text":"Enhanced documentation - includes new webpage: https://majkamichal.github.io/naivebayes/ naive_bayes() Poisson distribution now available model class conditional probabilities non-negative integer predictors. applied vectors class “integer” via new parameter usepoisson = TRUE. default usepoisson = FALSE. naive_bayes objects created previous versions fully compatible 0.9.6 version. predict.naive_bayes() new parameter eps specifies value epsilon-range replace zero close zero probabilities specified threshold. applies metric variables well discrete variables, laplace = 0. predict.naive_bayes() now efficient reliable. print() method enhanced better readability. plot() method allows now visualising class marginal class conditional distributions predictor variable via new parameter prob two possible values: \"marginal\" \"conditional\".","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"new-functions-0-9-6","dir":"Changelog","previous_headings":"","what":"New functions","title":"naivebayes 0.9.6","text":"bernoulli_naive_bayes() - specialised version naive_bayes(), features take 0-1 values feature modelled Bernoulli distribution. gaussian_naive_bayes() - specialised version naive_bayes(), features real valued feature modelled Gaussian distribution. poisson_naive_bayes() - specialised version naive_bayes(), features non-negative integers feature modelled Poisson distribution. nonparametric_naive_bayes() - specialised version naive_bayes(), features real valued distribution estimated kernel density estimation (KDE). multinomial_naive_bayes() - specialised Naive Bayes classifier suitable text classification. %class% %prob% - infix operators shorthands performing classification obtaining posterior probabilities, respectively. coef() - generic function extracts model coefficients specialized Naive Bayes objects. get_cond_dist() - obtaining names class conditional distributions assigned features.","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-095","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.5","title":"naivebayes 0.9.5","text":"CRAN release: 2019-03-17 Fixed: laplace > 0 discrete feature >2 distinct values, probabilities probability table sum 1.","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-094","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.4","title":"naivebayes 0.9.4","text":"CRAN release: 2019-03-10 Fixed: plot.naive_bayes() crashes missing data present training set (bug found Mark van der Loo).","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-093","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.3","title":"naivebayes 0.9.3","text":"CRAN release: 2019-01-07 Fixed: numerical underflow predict.naive_bayes function number features big (bug found William Townes). Fixed: names features newdata predict.naive_bayes() match defined naive_bayes object, calculation based prior probabilities done one row newdata. Improvement: better handling (informative warnings/errors) correct inputs predict.naive_bayes(). Improvement: print.naive_bayes() now transparent.","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-092","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.2","title":"naivebayes 0.9.2","text":"CRAN release: 2018-01-03 Fixed: data two classes alphabetically ordered, predicted classes incorrect (bug found Max Kuhn).","code":""},{"path":"https://majkamichal.github.io/naivebayes/news/index.html","id":"naivebayes-091","dir":"Changelog","previous_headings":"","what":"naivebayes 0.9.1","title":"naivebayes 0.9.1","text":"CRAN release: 2017-01-15 Fixed: prediction data one row, column names get dropped (bug found Max Kuhn).","code":""}]
